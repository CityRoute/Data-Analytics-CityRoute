{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59cdda17",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "[1. Model Preparation](#1.-Model-Preperation)\n",
    "<br>\n",
    "* [1.1 Reviewing, Splitting data set](#1.1-Reviewing,-splitting-dataset-into-7:3-for-training-and-testing.)\n",
    "* [1.2 Plotting features against target feature](#1.2-Plot-to-compare-all-features-to-target-feature-to-help-make-decisions-to-keep-for-the-models.)\n",
    "    * [1.2.1 Plotting datetime feature against target feature](#Plotting-datetime-feature-against-target-feature)\n",
    "    * [1.2.2 Plotting numerical features against target feature](#Plotting-numerical-features-against-target-feature)\n",
    "    * [1.2.3 Plotting categorical features against target feature](#Plotting-categorical-features-against-target-feature)\n",
    "* [1.3. Summary of all features](#1.3.-Summary-of-all-features)\n",
    "    * [1.3.1 Numerical Features](#Numerical-Features)\n",
    "    * [1.3.1 Cateogrical Features](#Categorical-Features)\n",
    "* [2. Linear Regression](#2.-Linear-Regression)\n",
    "* [3. Route model and taking the proportion of the prediction to calculate a journey time for the user](#3.-Route-model-and-taking-the-proportion-of-the-prediction-to-calculate-a-journey-time-for-the-user.)\n",
    "    * [3.1 Calculating the proportion of each stop from the overall trip](#3.1-Calculating-the-proportion-of-each-stop-from-the-overall-trip.)\n",
    "* [4. Stop pair model](#4.-Stop-pair-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d23755",
   "metadata": {},
   "source": [
    "Establishing a connection with sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad2ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import sqlite3\n",
    "import pickle\n",
    "\n",
    "# from sagemaker import get_execution_role\n",
    "from patsy import dmatrices\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from math import log\n",
    "from statistics import stdev\n",
    "from statistics import mode\n",
    "\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Connecting to s3\n",
    "# role = get_execution_role()\n",
    "# bucket='sagemaker-studio-520298385440-7in8n1t299'\n",
    "# data_key = 'route_46a.feather'\n",
    "# data_location = 's3://{}/{}'.format(bucket, data_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a62a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def function to create connection to db\n",
    "def create_connection(db_file):\n",
    "    \"\"\"\n",
    "    create a database connection to the SQLite database specified by db_file\n",
    "    :param df_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try: \n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except 'Error' as e:\n",
    "        print(e)\n",
    "        \n",
    "    return conn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb73c0",
   "metadata": {},
   "source": [
    "sumo = \"\"\"SELECT COUNT(*) from leavetimes\"\"\"\n",
    "s = conn.execute(sumo)\n",
    "for x in s:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db01e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create connection to db\n",
    "db_file = \"C:/Users/fayea/UCD/ResearchPracticum/Data-Analytics-CityRoute/dublinbus.db\"\n",
    "conn = create_connection(db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise query\n",
    "query = \"\"\"\n",
    "SELECT leavetimes.*, weather.*\n",
    "FROM leavetimes, weather\n",
    "WHERE TRIPID in  \n",
    "    (SELECT TRIPID\n",
    "    FROM trips\n",
    "    WHERE LINEID = '46A' AND DIRECTION = '1')\n",
    "AND leavetimes.DAYOFSERVICE = weather.dt;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query and read into dataframe\n",
    "query_df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec5b84",
   "metadata": {},
   "source": [
    "# 1. Model Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading file\n",
    "df = query_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefaac50",
   "metadata": {},
   "source": [
    "## 1.1 Reviewing, splitting dataset into 7:3 for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f523bc15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c18c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837dc6a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unique types for each feature\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57494ad6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Datatypes and convert\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2454b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1882786",
   "metadata": {},
   "source": [
    "**Review so far:**\n",
    "<br>\n",
    "There are no more missing values and the constant columns have been removed.\n",
    "* Remove index, index, dt.\n",
    "* Investigate level_0.\n",
    "* Convert the following to categorical: DAYOFWEEK, MONTHOFSERVICE, PROGRNUMBER, STOPPOINTID, VEHICLEID, IS_HOLIDAY, IS_WEEKDAY, TRIPID, weather_id, weather_main, weather_description\n",
    "* We have data for most of the days of the year and for each month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['index', 'level_0', 'dt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5377c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by trip then dayofservice\n",
    "df = df.sort_values(by=['TRIPID', 'DAYOFSERVICE', 'PROGRNUMBER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dffdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating features\n",
    "categorical_features = ['DAYOFWEEK', 'MONTHOFSERVICE', 'PROGRNUMBER', 'STOPPOINTID', 'PREVIOUS_STOPPOINTID',\n",
    "                       'IS_HOLIDAY', 'IS_WEEKDAY', 'TRIPID', 'VEHICLEID', 'weather_id', 'weather_main', 'weather_description']\n",
    "\n",
    "datetime_features = ['DAYOFSERVICE']\n",
    "\n",
    "numerical_features = ['PLANNEDTIME_ARR', 'ACTUALTIME_ARR', 'PLANNEDTIME_DEP', 'ACTUALTIME_DEP',\n",
    "                     'DWELLTIME', 'PLANNEDTIME_TRAVEL', 'temp', 'pressure', 'humidity', 'wind_speed', 'wind_deg', 'rain_1h', 'clouds_all']\n",
    "\n",
    "target_feat = 'ACTUALTIME_TRAVEL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a861f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting object to categorical\n",
    "for column in categorical_features:\n",
    "    df[column] = df[column].astype('category')\n",
    "    \n",
    "# Converting dayofservice to datetime\n",
    "df['DAYOFSERVICE'] = pd.to_datetime(df['DAYOFSERVICE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4189963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing PROGRNUMBER equal to 1 of ACTUALTIME_TRAVEL with 0\n",
    "df.loc[df['PROGRNUMBER'] == '1', 'ACTUALTIME_TRAVEL'] = 0\n",
    "df.loc[df['PROGRNUMBER'] == '1', 'PLANNEDTIME_TRAVEL'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d380d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['PLANNEDTIME_TRAVEL'] < 0, 'PLANNEDTIME_TRAVEL'] = 0\n",
    "df.loc[df['ACTUALTIME_TRAVEL'] < 0, 'ACTUALTIME_TRAVEL'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d173d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making new feature for previous stoppointid and let those with PROGRNUMBER = 1 to 0\n",
    "# df['PREVIOUS_STOPPOINTID'] = df['STOPPOINTID'].shift()\n",
    "# first_stop = {'0':'0'}\n",
    "# df['PREVIOUS_STOPPOINTID'] = df['PREVIOUS_STOPPOINTID'].cat.add_categories(first_stop)\n",
    "# df.loc[df['PROGRNUMBER'] == '1', 'PREVIOUS_STOPPOINTID'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa66e4",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Setting the target feature as _y and x_ as the remaining features in the dataframe. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(np.random.permutation(df.index))\n",
    "# sort the resulting random index\n",
    "df.sort_index(inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating y and x axis\n",
    "target_feature = df['ACTUALTIME_TRAVEL']\n",
    "y = pd.DataFrame(target_feature)\n",
    "X = df.drop(['ACTUALTIME_TRAVEL'], axis=1)\n",
    "\n",
    "# Splitting dataset for train and testing data by 70/30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Printing shape of the new split data\n",
    "print(\"The original range is: \",df.shape[0])\n",
    "print(\"The training range (70%):\\t rows 0 to\", round(X_train.shape[0]))\n",
    "print(\"The test range (30%): \\t rows\", round(X_train.shape[0]), \"to\", round(X_train.shape[0]) + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19da30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12306ab",
   "metadata": {},
   "source": [
    "## 1.2 Plot to compare all features to target feature to help make decisions to keep for the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e3e69",
   "metadata": {},
   "source": [
    "#### Plotting datetime feature against target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7390280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot datetime feature against target feature\n",
    "X_train.DAYOFSERVICE = pd.to_numeric(X_train.DAYOFSERVICE)\n",
    "df_temp = pd.concat([X_train['DAYOFSERVICE'], y_train], axis=1)\n",
    "correlation_dt = df_temp[['DAYOFSERVICE', 'ACTUALTIME_TRAVEL']].corr(method='pearson')\n",
    "correlation_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68c32f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot\n",
    "df_temp.plot(kind='scatter', x='DAYOFSERVICE', y='ACTUALTIME_TRAVEL', label = \"%.3f\" % df_temp[['ACTUALTIME_TRAVEL', 'DAYOFSERVICE']].corr().to_numpy()[0,1], figsize=(15, 8)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f23776a",
   "metadata": {},
   "source": [
    "#### Plotting numerical features against target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f930bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in numerical_features:\n",
    "    df_temp = pd.concat([X_train[column], y_train], axis=1)\n",
    "    correlation_dt = df_temp[[column, 'ACTUALTIME_TRAVEL']].corr(method='pearson')\n",
    "    print('\\n',correlation_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b2b0e",
   "metadata": {},
   "source": [
    "Using pearson correlation, we see that the correlation between "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e6f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in numerical_features:\n",
    "    df_temp = pd.concat([X_train[column], y_train], axis=1)\n",
    "    correlation_dt = df_temp[[column, 'ACTUALTIME_TRAVEL']].corr(method='spearman')\n",
    "    print('\\n',correlation_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2823d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in numerical_features:\n",
    "    df_temp = pd.concat([X_train[column], y_train], axis=1)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot\n",
    "    df_temp.plot(kind='scatter', x=column, y='ACTUALTIME_TRAVEL', label = \"%.3f\" % df_temp[['ACTUALTIME_TRAVEL', column]].corr(method='pearson').to_numpy()[0,1], figsize=(12, 8)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db9878f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in numerical_features:\n",
    "    df_temp = pd.concat([X_train[column], y_train], axis=1)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot\n",
    "    df_temp.plot(kind='scatter', x=column, y='ACTUALTIME_TRAVEL', label = \"%.3f\" % df_temp[['ACTUALTIME_TRAVEL', column]].corr(method='spearman').to_numpy()[0,1], figsize=(12, 8)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df[numerical_features]\n",
    "for feature in df_numeric:\n",
    "    df_numeric[feature] = np.log(df_numeric[feature])\n",
    "df_numeric['ACTUALTIME_TRAVEL'] = np.log(df['ACTUALTIME_TRAVEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6daf631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating y and x axis\n",
    "target_feature_numeric = df_numeric['ACTUALTIME_TRAVEL']\n",
    "y_numeric = pd.DataFrame(target_feature_numeric)\n",
    "X_numeric = df_numeric.drop(['ACTUALTIME_TRAVEL'], axis=1)\n",
    "\n",
    "# Splitting dataset for train and testing data by 70/30\n",
    "X_train_numeric, X_test_numeric, y_train_numeric, y_test_numeric = train_test_split(X_numeric, y_numeric, test_size=0.3, random_state=1)\n",
    "\n",
    "# Printing shape of the new split data\n",
    "print(\"The original range is: \",df.shape[0])\n",
    "print(\"The training range (70%):\\t rows 0 to\", round(X_train_numeric.shape[0]))\n",
    "print(\"The test range (30%): \\t rows\", round(X_train_numeric.shape[0]), \"to\", round(X_train_numeric.shape[0]) + X_test_numeric.shape[0])\n",
    "\n",
    "for column in numerical_features:\n",
    "    df_temp = pd.concat([X_train_numeric[column], y_train_numeric], axis=1)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot\n",
    "    df_temp.plot(kind='scatter', x=column, y='ACTUALTIME_TRAVEL', label = \"%.3f\" % df_temp[['ACTUALTIME_TRAVEL', column]].corr(method='spearman').to_numpy()[0,1], figsize=(12, 8)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417625f3",
   "metadata": {},
   "source": [
    "#### Plotting categorical features against target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df27e21d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year_features = ['DAYOFWEEK', 'IS_HOLIDAY', 'IS_WEEKDAY', 'MONTHOFSERVICE', 'weather_id', 'weather_main', 'weather_description']\n",
    "\n",
    "for feature in year_features:\n",
    "    print(feature)\n",
    "    df_temp = pd.concat([X_train, y_train], axis=1)\n",
    "    unique = df_temp[feature].unique()\n",
    "    list_average = []\n",
    "    \n",
    "    for value in unique:\n",
    "        list_values = df_temp[df_temp[feature]== value]['ACTUALTIME_TRAVEL'].tolist()\n",
    "        length_list = len(list_values)\n",
    "        average =  sum(list_values)/length_list\n",
    "        list_average += [average]\n",
    "#         print(f'Sum of values / list of values: \\n {sum(list_values)} / {length_list}')\n",
    "#         print(f'Average ACTUALTIME_TRAVEL: {average}, \\n')\n",
    "        \n",
    "    # taken from https://pythonspot.com/matplotlib-bar-chart/\n",
    "    y_pos = np.arange(len(unique))\n",
    "    plt.bar(y_pos, list_average, align='center')\n",
    "    plt.xticks(y_pos, unique)\n",
    "    plt.ylabel('Usage')\n",
    "    plt.title(feature)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50231ccf",
   "metadata": {},
   "source": [
    "I think there is some outliers in ACTUALTME_TRAVEL. The averages are all in negatives which suggests that the travel times. Would that be an outlier if the negative values are very great? \n",
    "<br><br>\n",
    "**DAYOFWEEK:**\n",
    "The lowest average is Sunday and the busiest is Monday. So it does make a difference.\n",
    "<br><br>\n",
    "**IS_WEEKDAY:**\n",
    "The same comment that there is a difference in average times.\n",
    "<br><br>\n",
    "**MONTHOFSERVICE:**\n",
    "Interestingly enough, they have difference averages depending on each month with August being the least busiest and April being the busiest. It must have something to do with the weather maybe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0401900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average time for each vehicle id\n",
    "df_temp = pd.concat([X_train, y_train], axis=1)\n",
    "vehicleid = df_temp['VEHICLEID'].unique().tolist()\n",
    "for id_ in vehicleid:\n",
    "    print(f'VEHICLEID: {id_}')\n",
    "    list_values = df_temp[df_temp['VEHICLEID']== id_]['ACTUALTIME_TRAVEL'].tolist()\n",
    "    length_list = len(list_values)\n",
    "    average =  sum(list_values)/length_list\n",
    "    print(f'Average ACTUALTIME_TRAVEL: {average} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a36ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dummy variables for categorical \n",
    "cat = ['DAYOFWEEK', 'MONTHOFSERVICE', 'PROGRNUMBER', 'STOPPOINTID', 'IS_HOLIDAY', 'IS_WEEKDAY', 'weather_id', 'weather_main', 'weather_description']\n",
    "df_temp = pd.concat([X_train, y_train], axis=1)\n",
    "df_copy = df_temp.copy()\n",
    "df_copy = df_copy[cat]\n",
    "df_copy = pd.get_dummies(df_copy)\n",
    "df_copy = pd.concat([df_copy, y_train], axis=1)\n",
    "\n",
    "categorical_corr = df_copy.corr()['ACTUALTIME_TRAVEL'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddbe54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(categorical_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a1aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_list = categorical_corr[categorical_corr > 0.04].index.tolist()\n",
    "categorical_list.remove('ACTUALTIME_TRAVEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0462f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categorical_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e0f51",
   "metadata": {},
   "source": [
    "## 1.3. Summary of all features\n",
    "<br><br>\n",
    "#### Numerical Features\n",
    "<br><br>\n",
    "\n",
    "**DayOfService:**\n",
    "* The correlation to the target feature is very low of 0.03806.\n",
    "* Don't see it being a useful feature for the target feature. \n",
    "* Plot represents a straight line, which suggests little to no correlation.\n",
    "* Conclusion: dropped because of the low correlation score. \n",
    "\n",
    "**PlannedTime_Arr:**\n",
    "* There is very low correlation against the target feature though it gets better using spearman correlation.\n",
    "* After logging the data, the correlation plot did not make a huge difference when using the spearman method to plot it for the second time. \n",
    "* Pearson and spearman plot pre log suggests little correlation as it is a continuous straight line. However, this shouldn't mean it should be dropped.\n",
    "* When most values in the target feature fell less than 10, we see that the plannedtime arrival values increasing, it didn't change much. This would be due to the fact that the target feature is the difference between times so it would make sense that the relationship is poor.\n",
    "* After logging the data, the plot is more spread out instead of a straight line, but the correlation score still shows a similar low score with a .02 difference using the spearman method. \n",
    "* Conclusion: However, this will be dropped.\n",
    "\n",
    "**ActualTime_Arr:**\n",
    "* Compared to Planned time arrival feature, the pearson correlation score is poorer but the spearman scores are more similar pre log. \n",
    "* It is similar to planned time arrival in that the plot represents a straight line, that suggests a poor relationship with the target feature. \n",
    "* After logging the data, it is found that the plot is more spread out. The score using spearman is not much different pre logging the data. \n",
    "* However, it would be unwise to drop this feature as it I feel it would serve good purpose for the target feature for predicting the prediction time for the next stop. \n",
    "* Conclusion: this will be dropped.\n",
    "\n",
    "**PlannedTime_Dep:**\n",
    "* Planned time departure has little correlation with the target feature after looking at spearman and pearsons. \n",
    "* It doesn't have a linear relationship and the straight line on the plot of both methods proves this.\n",
    "* However, when plotted using the logged values we see that the correlation score hasn't changed but the data is more spread out. \n",
    "* This doesn't change the relationship much, however. \n",
    "* Even so, this will be kept as I feel it would help the predictions. Having the planned time departures would help skew a better result because it would relatively be close to the actual time departure even though it is just an estimate.\n",
    "* Conclusion: this will be dropped \n",
    "\n",
    "**ActualTime_Dep:**\n",
    "* Actual time departure is again, more or less the same. It represents the departure for these times at a particular stop to go to the next stop. It is strange that the correlation is so low even after logging the data but it would make sense as you wouldn't expect there to be a linear relationship.\n",
    "* The plot is similar to the rest of the previous features mentioned so far. \n",
    "* However, it will still be kept because I feel it would still be a useful feature for predicting a time in seconds. \n",
    "* By taking the actual time departure for a particular stop it may help.\n",
    "* Conclusion: this will be dropped.\n",
    "\n",
    "**Dwell Time:**\n",
    "* Dwell time has a 0.03 coorelation score with the target feature. It suggests on the graph that the time for dwell time equal to 0 then the more the target feature time increases. It might suggest traffic times where if a bus is full then it might be due to rush hour? busy hours?\n",
    "* Plotting against the target feature after logging the data gives similar scores using the spearman correlation method. However we see the graph differing from pre log plot. It is more grouped up together compared to the previous graph plot.\n",
    "* Because the score is more fairer compared to the previous, it will be useful to keep it for the modelling.\n",
    "* Conclusion: dropped.\n",
    "\n",
    "**PlannedTime_Travel:**\n",
    "* When plotting using the pearse correlation method, it gave a correlation of 0.2. This time it is the highest correlation and we see a small linear relationship.\n",
    "* The time for planned time travel, as it increases, so does the target feature. It gives us an indication of that slight linear relationship.\n",
    "* Using spearmans to graph the correlation gave us a 0.7 score which is a good indication that the two features has a linear relationship.\n",
    "* Because of this, this feature will be dropped.\n",
    "\n",
    "**Temp:**\n",
    "* Temp  has a negative 0.009 correlation with the target feature and an even poorer linear relationship at -.002.\n",
    "* This indicates a poor linear/monotonic relationship and it will not serve useful for the model.\n",
    "* The graph plots does not give anymore useful information that would give further evidence that it should be kept.\n",
    "* Conclusion: drop.\n",
    "\n",
    "**Pressure:**\n",
    "* It also has a negative linear relationship with the target feature.\n",
    "* When looking at the graph plots for both spearman and pearsons, it does not give any further insights.\n",
    "* For this reason, this feature will be dropped.\n",
    "\n",
    "**Humidity:**\n",
    "* Humidity does not have a strong relationship with the target feature, be it linear or monotonic.\n",
    "* The reason being the correlation using both methods fell < 0.00. \n",
    "* Unfortunately, the graph does not represent anything useful either.\n",
    "* When looking at the logged data plots however, there is a slight difference however it is not signficant enough that this feature should still be kept as there is no distinct relationship that we can see.\n",
    "* Conclusion: drop.\n",
    "\n",
    "**Windspeed:**\n",
    "* No linear relationship.\n",
    "* Indicates a small monotonic relationship.\n",
    "* This means that as the windspeed value increases, the value of the target feature tends to be higher as well.\n",
    "* But a spearman correlation of 0.01 is not strong enough of a feature to keep.\n",
    "* Conclusion: drop\n",
    "\n",
    "**Wind_Deg:**\n",
    "* This feature will be dropped immediately as the correalations are both <0.000.\n",
    "\n",
    "**Rain_1H:**\n",
    "* It doesn't have a strong linear relationship but it shows spearmans correlation some promising results when the data has been logged.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "#### Categorical Features\n",
    "<br><br>\n",
    "**DayOfWeek:**\n",
    "* In the graph we see the actual time travel increasing during weekdays and slowly the travel time is less during weekends. \n",
    "* This suggests a relationship between the days of the week and the target feature in which weekdays have a higher tendency for the actualtime travel feature to be higher.\n",
    "* Conclusion: this will be kept.\n",
    "\n",
    "**MonthofService:**\n",
    "* In the graph, we don't really see a connection between each month against the target feature even if it is in order. \n",
    "* The overall actual travel time is higher in february before it dips, then rising during winter season.\n",
    "* The correlation score seems to be poor also for each month. \n",
    "* This feature will still be kept. \n",
    "\n",
    "**Progrnumber:**\n",
    "* Most progrnumbers will be dropped as a lot of the correlations are <0.00.\n",
    "* For this reason, this feature will be dropped.\n",
    "    \n",
    "**StoppointID:**\n",
    "* Similarly to progrnumbers, there are a lot of low correlations falling <0.00.\n",
    "* Most stoppoint numbers are <0.00 correlation.\n",
    "* This indicates a very low relationship with the target feature. \n",
    "* For this reason, this feature will be dropped, except for those with a correlation > 0.04\n",
    "    \n",
    "**Is_Holiday:**\n",
    "* After analyzing the graph, we see a relationship between the target feature and whether or not the time falls under a holiday date (non-school holiday).\n",
    "* If it a non holiday, the actual time travel increases. \n",
    "* If it is a holiday, the actual time travel decreases. \n",
    "* This means that less people are using public transport if it is a holiday date.\n",
    "* For this reason, this feature will be kept.\n",
    "\n",
    "**Is_Weekday:**\n",
    "* Like Is_Holiday, we see a relationship between the target feature and whether or not the time is during a weekday or not. \n",
    "* We see a contrast between the two values in which 1, being a weekday, has a higher actual time travel, vice versa.\n",
    "* For this reason, it is a good indication of a relationship to the target feature.\n",
    "* Therefore, this feature will be kept. \n",
    "\n",
    "**VehicleID:**\n",
    "* When looking at the different averages, we see that the average differences are not big.\n",
    "* For this reason, it may be best to drop this feature because it doesn't give any indication it would be a useful feature to help the prediction models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5cfc3e",
   "metadata": {},
   "source": [
    "## 1.4 Cleaning up features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60620df",
   "metadata": {},
   "source": [
    "### Setting low correlation features - keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c417f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "low_corr_categorical = ['DAYOFWEEK', 'MONTHOFSERVICE', 'IS_HOLIDAY', 'IS_WEEKDAY'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cb7079",
   "metadata": {},
   "source": [
    "### Setting low correlation features - drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features\n",
    "low_corr_numerical = ['PLANNEDTIME_ARR', 'PLANNEDTIME_DEP', 'ACTUALTIME_ARR', 'ACTUALTIME_DEP','PLANNEDTIME_TRAVEL']\n",
    "\n",
    "low_corr = ['DAYOFSERVICE', 'VEHICLEID', 'TRIPID', 'STOPPOINTID', 'PREVIOUS_STOPPOINTID', 'PROGRNUMBER', 'temp', 'pressure', 'humidity', \n",
    "            'wind_speed', 'wind_deg', 'weather_id', 'weather_description', 'clouds_all', 'PREVIOUS_STOPPOINTID', 'PLANNEDTIME_ARR', 'PLANNEDTIME_DEP', 'ACTUALTIME_ARR', 'ACTUALTIME_DEP',\n",
    "           'PLANNEDTIME_TRAVEL', 'DWELLTIME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3a89ee",
   "metadata": {},
   "source": [
    "### Setting high correlation  features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e969a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features \n",
    "high_corr_numerical = ['DWELLTIME', 'PLANNEDTIME_TRAVEL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d2ffa",
   "metadata": {},
   "source": [
    "### Dropping features & setting dummy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ee73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_copy = df_copy.drop(low_corr, 1)\n",
    "df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df258c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_copy = pd.get_dummies(df_copy)\n",
    "df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2fab3f",
   "metadata": {},
   "source": [
    "### Training & Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a18a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All features\n",
    "features = df_copy.columns.tolist()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c133c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = {'ACTUALTIME_TRAVEL': df_copy['ACTUALTIME_TRAVEL']}\n",
    "y = pd.DataFrame(data=datas)\n",
    "X = df_copy.drop(['ACTUALTIME_TRAVEL'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe715dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into 2 datasets: \n",
    "# Split the dataset into two datasets: 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)\n",
    "\n",
    "print(\"The Original range of the dataset: \",df.shape[0])\n",
    "print(\"The Training range taken from dataset: (70%): rows 0 to\", round(X_train.shape[0]))\n",
    "print(\"The Test range taken from dataset: (30%): rows\", round(X_train.shape[0]), \"to\", round(X_train.shape[0]) + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7780b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nDescriptive features in X:\\n\", X_train.head(5))\n",
    "print(\"\\nTarget feature in y:\\n\", y_train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e924e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will reset the indexes of the training and test splits so we can see the X_train printout\n",
    "# We will see that they are no longer in order and the next markdown cell I will reset the indexes.\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5ce811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .reset_index \n",
    "# We see that they are in order again. \n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad45e8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87872df1",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# 2. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a704e0",
   "metadata": {},
   "source": [
    "In this section, I will be preparating a linear regression model. I will attempt to see the prediction overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9381bfc",
   "metadata": {},
   "source": [
    "## 2.1 Training a linear regression model to predict the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4dd3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e835234f",
   "metadata": {},
   "source": [
    "## 2.2 Printing out the coefficients learned by the model and discussing the role in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ead451",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nThe features are: \\n\", X_train.columns)\n",
    "print(\"\\nThe coefficients are: \\n\", linear_reg.coef_)\n",
    "print(\"\\n The intercept is: \\n\", linear_reg.intercept_)\n",
    "print(\"\\nFeatures and coefficients: \\n\", list(zip(X_train.columns, linear_reg.coef_[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4871d53",
   "metadata": {},
   "source": [
    "## 2.3 Printing the predicted target feature. Printing the predicted class for a few examples. Printing classification evaluation measures computed on the full training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d355128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the prediction and threshold value. \n",
    "linear_predictions_train_data = (linear_reg.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576d910",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted_multiplelinreg = pd.concat([y_train, pd.DataFrame(linear_predictions_train_data, columns=['Predicted'])], axis=1)\n",
    "print(actual_vs_predicted_multiplelinreg.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75642d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing the a few classification evaluation measures computed on the full training set.\n",
    "# The following will be printed: accuracy, confusion matrix, precision, recall, f1).\n",
    "# Some more evaluation metrics.\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(y_train, linear_predictions_train_data)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(y_train, linear_predictions_train_data))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(y_train, linear_predictions_train_data))\n",
    "print(\"R2 Score: \", metrics.r2_score(y_train, linear_predictions_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b03ed7",
   "metadata": {},
   "source": [
    "## 2. 4 Evaluating the model using classification evaluation measures on the hold-out (30% examples) test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9446294",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_predictions_testing = (linear_reg.predict(X_test))\n",
    "\n",
    "print(\"\\nPredictions that has multiple linear regression: \\n\")\n",
    "actual_vs_predicted_multiple_linear_reg = pd.concat([y_test, pd.DataFrame(linear_predictions_testing, columns=['Predicted'])], axis=1)\n",
    "print(actual_vs_predicted_multiple_linear_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6686dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the a few classification evaluation measures computed on the full training set.\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(y_test, linear_predictions_testing)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(y_test, linear_predictions_testing))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(y_test, linear_predictions_testing))\n",
    "print(\"R2 Score: \", metrics.r2_score(y_test, linear_predictions_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e862b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a tripid from 46a and applying a prediction using Linear Regression model.\n",
    "route_46a = df[(df['TRIPID'] == '8591174') & (df['DAYOFSERVICE']=='2018-12-23')]\n",
    "route_46a = route_46a.drop(low_corr, 1)\n",
    "route_46a = pd.get_dummies(route_46a)\n",
    "actualtimes_46a = pd.DataFrame(route_46a['ACTUALTIME_TRAVEL'])\n",
    "actualtimes_46a.reset_index(drop=True, inplace=True)\n",
    "route_46a = route_46a.drop('ACTUALTIME_TRAVEL', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b3c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_46a = linear_reg.predict(route_46a)\n",
    "prediction_46a[0] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f72b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_46a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c460137f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted = pd.concat([actualtimes_46a, pd.DataFrame(prediction_46a, columns=['Prediction'])], axis=1, join='outer')\n",
    "actual_vs_predicted.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e66dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actualtimes_46a, prediction_46a)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actualtimes_46a, prediction_46a))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actualtimes_46a, prediction_46a))\n",
    "print(\"R2 Score: \", metrics.r2_score(actualtimes_46a, prediction_46a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd7cee",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4f42c",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa91a8",
   "metadata": {},
   "source": [
    "# 3. Route model and taking the proportion of the prediction to calculate a journey time for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa8cd2",
   "metadata": {},
   "source": [
    "## 3.1 Calculating the proportion of each stop from the overall trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb07bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_stops(predictions):\n",
    "    # Sum from the first stop until each stop\n",
    "    sum_each_stop = np.zeros(predictions.shape[0], dtype=float)\n",
    "    proportion_each_stop = np.zeros(predictions.shape[0], dtype=float)\n",
    "    overall_prediction = np.sum(predictions)\n",
    "    \n",
    "    # Adding sum up until current stop and dividing by overall prediction to get proportion of the trip\n",
    "    for length in range(predictions.shape[0]):\n",
    "        sum_each_stop = np.append(sum_each_stop, [predictions[length]])\n",
    "        sum_overall = np.sum(sum_each_stop) / overall_prediction*100\n",
    "        proportion_each_stop[length] = sum_overall\n",
    "        \n",
    "    return proportion_each_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9687e1",
   "metadata": {},
   "source": [
    "## 3.2 Return the progrnumber based off the stoppointid in a route"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbcb809",
   "metadata": {},
   "source": [
    "Finding the most common progrnumber based off the stoppointid. The reason for using to find the most common progrnumber is because it assumes that most route_id for each line would be always complete with the exception of a few trips in which they take a different route and skips some stops as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e8cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from https://www.geeksforgeeks.org/python-find-most-frequent-element-in-a-list/\n",
    "\n",
    "# array only accepts a panda Series or numpy array\n",
    "def most_common(array):\n",
    "    List = array.tolist()\n",
    "    mode_list = mode(List)\n",
    "    if mode_list == '1':\n",
    "        return 0\n",
    "    \n",
    "    else:\n",
    "        return(mode(List))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da87ce03",
   "metadata": {},
   "source": [
    "## 3.3 Calculating the journey time from a start to end destination based on user input\n",
    "\n",
    "Finding the travel time duration based on a stoppointid then getting the progrnumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e56f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def journey_time(start,end, prediction):\n",
    "    # Converting into int because the function returns a string\n",
    "    start_progrnum = int(most_common(df['PROGRNUMBER'][df['STOPPOINTID']==start]))\n",
    "    end_progrnum = int(most_common(df['PROGRNUMBER'][df['STOPPOINTID']==end]))\n",
    "    \n",
    "#     print(start_progrnum)\n",
    "#     print(end_progrnum)\n",
    "\n",
    "    proportion_array = proportion_stops(prediction)\n",
    "    overall_prediction = np.sum(prediction)\n",
    "    \n",
    "    # calculating the time difference from start to end destination \n",
    "    start_prediction = (proportion_array[start_progrnum]/100) * overall_prediction\n",
    "    end_prediction = (proportion_array[end_progrnum]/100) * overall_prediction\n",
    "    \n",
    "    journeytime = end_prediction - start_prediction\n",
    "    \n",
    "    # print(journeytime)\n",
    "    \n",
    "    return journeytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84041e62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_start = '807'\n",
    "user_end = '812'\n",
    "\n",
    "journey_time(user_start, user_end, prediction_46a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c799d97a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824c9ccb",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# 4. Stop pair model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd75e00",
   "metadata": {},
   "source": [
    "## 4.1 Pairing each stop based on the route - this approach has been disregarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b3eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing one trip \n",
    "first_trip = df[df['TRIPID']=='5955251']\n",
    "list_stopid = first_trip['STOPPOINTID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f07ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairing_stops(stops_df):\n",
    "    list_stopid = stops_df['STOPPOINTID'].tolist()\n",
    "    \n",
    "    # Creating pair stops for a route\n",
    "    paired_stop = np.array(list(zip(list_stopid, list_stopid[1:] + list_stopid[:1])), dtype=int)\n",
    "\n",
    "    # Removing last element from list (because it is a pair of the first and last stopid)\n",
    "    paired_stop = paired_stop[:-1]\n",
    "    \n",
    "    return paired_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_paired_stop(user_input, paired_stops):\n",
    "    stops = []\n",
    "    for i in range(len(paired_stops)):\n",
    "        if paired_stops[i][1] == user_input:\n",
    "#             print(user_input, paired_stops[i])\n",
    "            stops += paired_stops[i]\n",
    "    return stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53716bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stops_df(input_stops):\n",
    "    for first, second in pair_stop:\n",
    "        print(first, second)\n",
    "#         temp_df = df[df['STOPPOINTID'] == first]\n",
    "#         temp_df = temp_df.append(df[df['STOPPOINTID']== second])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7487941",
   "metadata": {},
   "source": [
    "## 4.2 First version of paired stop approach\n",
    "<br><br>\n",
    "This approach makes a model based on the stopid and its previous stopids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c21bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a paired list of stops\n",
    "def paired_stops(df):\n",
    "    stopid = df['STOPPOINTID'].unique().tolist()\n",
    "    previous_stopid = []\n",
    "    for i in stopid:\n",
    "        prev = df['PREVIOUS_STOPPOINTID'][df['STOPPOINTID']==i]\n",
    "        # Adds most frequent previous stopid to list\n",
    "        previous_stopid += [prev.value_counts().idxmax()]\n",
    "    \n",
    "    return [stopid, previous_stopid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5445cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in range(len(paired_stops[0])):\n",
    "    \n",
    "    # Making new dataframe\n",
    "    to_add = df[df['STOPPOINTID']==paired_stops[0][ids]]\n",
    "    to_add = to_add.append(df[df['PREVIOUS_STOPPOINTID']==paired_stops[1][ids]])\n",
    "    stops_df = pd.DataFrame(data=to_add)\n",
    "    \n",
    "    # Setting target feature\n",
    "    y = stops_df['ACTUALTIME_TRAVEL']\n",
    "    \n",
    "    # Dropping target feature and low corr features\n",
    "    stops_df = stops_df.drop(low_corr,1)\n",
    "    stops_df = stops_df.drop('ACTUALTIME_TRAVEL',1)\n",
    "    stops_df = pd.get_dummies(stops_df)\n",
    "    \n",
    "    # Fitting model based on stops\n",
    "    linear_reg = LinearRegression().fit(stops_df, y)\n",
    "    \n",
    "    # Save to pickle file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd93db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_stops = paired_stops(df)\n",
    "to_add = df[df['STOPPOINTID']==pair_stops[0][5]]\n",
    "to_add = to_add.append(df[df['PREVIOUS_STOPPOINTID']==pair_stops[1][5]])\n",
    "stops_df = pd.DataFrame(to_add)\n",
    "\n",
    " # Setting target feature\n",
    "y = stops_df['ACTUALTIME_TRAVEL']\n",
    "    \n",
    "# Dropping target feature and low corr features\n",
    "stops_df = stops_df.drop(low_corr,1)\n",
    "stops_df = stops_df.drop('ACTUALTIME_TRAVEL',1)\n",
    "stops_df = pd.get_dummies(stops_df)\n",
    "\n",
    "# Fitting/Training model based on stops\n",
    "linear_reg_model_ = LinearRegression().fit(stops_df, y)\n",
    "\n",
    "# Saving to pickle File\n",
    "with open('model_'+pair_stops[0][5]+'.pkl', 'wb') as handle:\n",
    "    pickle.dump(linear_reg_model_, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f521b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampledf = stops_df.iloc[[0]]\n",
    "sample_prediction = linear_reg_sample.predict(sampledf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ede36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb71b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_'+pair_stops[0][5]+'.pkl', 'rb') as handle:\n",
    "    model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(sampledf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e5f48",
   "metadata": {},
   "source": [
    "## 4.2.1 Setting up for 46a stop pair models using first approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3aabfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get previous stopid and return a paired list\n",
    "def pair_stopids(current_stopids):\n",
    "    previous_stopid = []\n",
    "    for i in current_stopids:\n",
    "        prev = df['PREVIOUS_STOPPOINTID'][df['STOPPOINTID']==i]\n",
    "        # Adds most frequent previous stopid to list\n",
    "        previous_stopid += [prev.value_counts().idxmax()]\n",
    "    \n",
    "    return [current_stopids, previous_stopid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a225876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the json file\n",
    "import json\n",
    "file = open('routes_and_stops.json',)\n",
    "routes_stops = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all stops for 46a going outbound ('1')\n",
    "list_46a_stops = routes_stops['46A']['outbound']\n",
    "\n",
    "# Pairing stopids and prev stopids from 46a route\n",
    "pairing_46a_stopids = pair_stopids(list_46a_stops)\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361a2ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ids in range(len(pairing_46a_stopids[0])):\n",
    "    # Making new dataframe\n",
    "    to_add = df[df['STOPPOINTID']==pairing_46a_stopids[0][ids]]\n",
    "    to_add = to_add.append(df[df['PREVIOUS_STOPPOINTID']==pairing_46a_stopids[1][ids]])\n",
    "    stops_df = pd.DataFrame(data=to_add)\n",
    "    \n",
    "    # Setting target feature\n",
    "    y = stops_df['ACTUALTIME_TRAVEL']\n",
    "    \n",
    "    # Dropping target feature and low corr features\n",
    "    stops_df = stops_df.drop(low_corr,1)\n",
    "    stops_df = stops_df.drop('ACTUALTIME_TRAVEL',1)\n",
    "    stops_df = pd.get_dummies(stops_df)\n",
    "    \n",
    "    # Fitting model based on stops\n",
    "    linear_reg_model = LinearRegression().fit(stops_df, y)\n",
    "    \n",
    "      # Save to pickle file\n",
    "#     with open('model_'+pairing_46a_stopids[0][ids]+'.pkl', 'wb') as handle:\n",
    "#         pickle.dump(linear_reg_model, handle)\n",
    "\n",
    "     # Predicting data\n",
    "    with open('stop_'+pair_stops[0][ids]+'.pkl', 'rb') as handle:\n",
    "        model = pickle.load(handle)\n",
    "    \n",
    "    k = model.predict(route_46a.iloc[[index]])\n",
    "    predictions += [k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actualtimes_46a, predictions)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actualtimes_46a, predictions))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actualtimes_46a, predictions))\n",
    "print(\"R2 Score: \", metrics.r2_score(actualtimes_46a, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9191c4c2",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "##### Conclusion:\n",
    "Linear regression model is not very good. MSE score is off by more than 1000 seconds. And the R2 score is at a negative value. This means the parameters need to be tuned. Keeping dwelltime might be good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ee248",
   "metadata": {},
   "source": [
    "## 4.3 Stop pair based on entire leavetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496fd346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the json file\n",
    "import json\n",
    "file = open('routes_and_stops.json',)\n",
    "routes_stops = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf894a78",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "1) Make a rough query that selects rows that contain a certain stopid and its previous stopid based on the direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df275b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise query - for OUTBOUND (WHERE DIRECTION == '1')\n",
    "query_stopid = \"SELECT leavetimes.*, weather.* FROM leavetimes, weather WHERE leavetimes.STOPPOINTID = \" + current_stopid + \" AND leavetimes.DAYOFSERVICE = weather.dt\"\n",
    "query_stopid_df = pd.read_sql(query_previoustop, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3808b364",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "2) Make a function that will combine lists in a list together as one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfb17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_listsoflist(to_combine):\n",
    "    combined = []\n",
    "    for each_list in to_combine:\n",
    "        combined += each_list\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912457d8",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "4) Make a function that will get rid of the duplicates in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29c81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique(stopids_list):\n",
    "    return list(set(stopids_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b251f7",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "5) Make a list to store all stopids for DIRECTION == outbound/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38502f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through every lineid, outbound \n",
    "stopids_outbound = []\n",
    "for i,j in routes_stops.items():\n",
    "    try:\n",
    "#         print(i, '\\n', routes_stops[i]['outbound'], '\\n')\n",
    "        stopids_outbound += [routes_stops[i]['outbound']]\n",
    "    except KeyError:\n",
    "        continue\n",
    "        \n",
    "# Calling function to get combined list\n",
    "combined_stopids_outbound = combine_listsoflist(stopids_outbound)\n",
    "\n",
    "# Calling function to get unique stopids from combined list\n",
    "unique_stopids_outbound = get_unique(combined_stopids_outbound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c50be",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "6) Make a list to store all stopids for DIRECTION ==inbound/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b4def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through every lineid, inbound\n",
    "stopids_inbound = []\n",
    "for i,j in routes_stops.items():\n",
    "    try:\n",
    "#         print(i, '\\n', routes_stops[i]['inbound'], '\\n')\n",
    "        stopids_inbound += [routes_stops[i]['inbound']]\n",
    "    except KeyError:\n",
    "        continue\n",
    "        \n",
    "# Calling function to get combined list\n",
    "combined_stopids_inbound = combine_listsoflist(stopids_inbound)\n",
    "\n",
    "# Calling function to get unique stopids from combined list - using set() to get rid off existing stops from outbound stops\n",
    "unique_stopids_inbound = list(set(combined_stopids_inbound) - set(combined_stopids_outbound))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d2bbc",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "7) Query to select all of the previous_stopids based on the current stopid and put it to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77617600",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_previoustop = \"SELECT leavetimes.PREVIOUS_STOPPOINTID FROM leavetimes WHERE leavetimes.STOPPOINTID = \" + current_stopid \n",
    "query_prevstop_df = pd.read_sql(query_previoustop, conn)\n",
    "\n",
    "# Converting into a pandas series then to list\n",
    "query_prevstop_series = query_prevstop_df.iloc[0]\n",
    "query_prevstop_list = query_prevstop_series.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5523b8",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "8) Query to select the rows based on the previous stopids and append them to the current dataframe of the current stopid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85232f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_prev_stops(query_prevstop_list):\n",
    "    df_final = pd.DataFrame()\n",
    "    # Loop through the list\n",
    "#     for previous_stopid in query_prevstop_list:\n",
    "    query_prevstop_rows = \"SELECT leavetimes.*, weather.* FROM leavetimes, weather WHERE leavetimes.PREVIOUS_STOPPOINTID IN \" + str(query_prevstop_list) + \" AND leavetimes.DAYOFSERVICE = weather.dt\"\n",
    "    df_prevstop = pd.read_sql(query_prevstop_rows, conn)\n",
    "    df_final = df_final.append(df_prevstop)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4be67c",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "9) Adding index on STOPPOINTID and PREVIOUS_STOPPOINTID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6793df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding indexes\n",
    "add_index1 = \"\"\"CREATE INDEX stopid ON leavetimes(STOPPOINTID);\"\"\"\n",
    "add_index2 = \"\"\"CREATE INDEX previous_stopid ON leavetimes(PREVIOUS_STOPPOINTID);\"\"\"\n",
    "conn.execute(add_index1)\n",
    "conn.execute(add_index2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d967e8f3",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "10) Piecing every step together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import pickle\n",
    "\n",
    "# from sagemaker import get_execution_role\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from math import log\n",
    "\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Connecting to s3\n",
    "# role = get_execution_role()\n",
    "# bucket='sagemaker-studio-520298385440-7in8n1t299'\n",
    "# data_key = 'route_46a.feather'\n",
    "# data_location = 's3://{}/{}'.format(bucket, data_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92453e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_corr = ['DAYOFSERVICE', 'VEHICLEID', 'TRIPID', 'STOPPOINTID', 'PREVIOUS_STOPPOINTID', 'PROGRNUMBER', 'temp', 'pressure', 'humidity', \n",
    "            'wind_speed', 'wind_deg', 'weather_id', 'weather_description', 'clouds_all', 'PREVIOUS_STOPPOINTID', 'PLANNEDTIME_ARR', 'PLANNEDTIME_DEP', 'ACTUALTIME_ARR', 'ACTUALTIME_DEP',\n",
    "           'PLANNEDTIME_TRAVEL', 'DWELLTIME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c624b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def function to create connection to db\n",
    "def create_connection(db_file):\n",
    "    \"\"\"\n",
    "    create a database connection to the SQLite database specified by db_file\n",
    "    :param df_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try: \n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except 'Error' as e:\n",
    "        print(e)\n",
    "        \n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create connection to db\n",
    "db_file = \"C:/Users/fayea/UCD/ResearchPracticum/Data-Analytics-CityRoute/dublinbus.db\"\n",
    "conn = create_connection(db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a0341f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Outbound\n",
    "# test_df = pd.DataFrame()\n",
    "\n",
    "for current_stopid in range(len(stopids_outbound[0])):\n",
    "    # Making query to db and make df\n",
    "    query_stopid = \"SELECT leavetimes.*, weather.* FROM leavetimes, weather WHERE leavetimes.STOPPOINTID = \" + stopids_outbound[0][current_stopid] + \" AND leavetimes.DAYOFSERVICE = weather.dt\"\n",
    "    df = pd.read_sql(query_stopid, conn)\n",
    "\n",
    "    # Get all previous stopids in list\n",
    "    query_previoustop = \"SELECT leavetimes.PREVIOUS_STOPPOINTID FROM leavetimes WHERE leavetimes.STOPPOINTID = \" + stopids_outbound[0][current_stopid] \n",
    "    query_prevstop_df = pd.read_sql(query_previoustop, conn)\n",
    "\n",
    "    # Converting into a pandas series then to list\n",
    "    query_prevstop_series = query_prevstop_df['PREVIOUS_STOPPOINTID'].tolist()\n",
    "#     print(query_prevstop_series)\n",
    "#     query_prevstop_list = query_prevstop_series.tolist()\n",
    "#     print(query_prevstop_list)\n",
    "    query_prevstop_list = [stopid for stopid in query_prevstop_series if stopid != '0']\n",
    "    query_prevstop_list = tuple(get_unique(query_prevstop_list))\n",
    "    \n",
    "    # Append previous stops rows to main df\n",
    "    to_add = df_prev_stops(query_prevstop_list)\n",
    "    df = df.append(to_add)\n",
    "    \n",
    "    print('Concatentaion finished')\n",
    "    \n",
    "    # Drop low correlated features and setting target feature\n",
    "    df = df.drop(low_corr, 1)\n",
    "    tf = df['ACTUALTIME_TRAVEL']\n",
    "    df = df.drop('ACTUALTIME_TRAVEL', 1)\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    # Fitting model based on stops\n",
    "    linear_reg_model = LinearRegression().fit(df, tf)\n",
    "    \n",
    "    # Save to pickle file\n",
    "    with open('/UCD/ResearchPracticum/Data-Analytics-CityRoute/stop_models/stop_'+ stopids_outbound[0][current_stopid] +'.pkl', 'wb') as handle:\n",
    "        pickle.dump(linear_reg_model, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inbound \n",
    "for current_stopid in range(len(stopids_inbound)):\n",
    "    # Making query to df and make df\n",
    "    query_stopid = \"SELECT leavetimes.*, weather.* FROM leavetimes, weather WHERE leavetimes.STOPPOINTID = \" + stopids_inbound[current_stopid] + \" AND leavetimes.DAYOFSERVICE = weather.dt\"\n",
    "    df = pd.read_sql(query_previoustop, conn)\n",
    "    \n",
    "    # Get all previous stopids in list\n",
    "    query_previoustop = \"SELECT leavetimes.PREVIOUS_STOPPOINTID FROM leavetimes WHERE leavetimes.STOPPOINTID = \" + stopids_inbound[current_stopid] \n",
    "    query_prevstop_df = pd.read_sql(query_previoustop, conn)\n",
    "\n",
    "    # Converting into a pandas series then to list\n",
    "    query_prevstop_series = query_prevstop_df.iloc[0]\n",
    "    query_prevstop_list = query_prevstop_series.tolist()\n",
    "    \n",
    "    # Append previous stops rows to main df\n",
    "    to_add = df_prev_stops(query_prevstop_list)\n",
    "    df = df.append(to_add)\n",
    "    \n",
    "    # Drop low correlated features and setting target feature\n",
    "    df = df.drop(low_corr, 1)\n",
    "    tf = df['ACTUALTIME_TRAVEL']\n",
    "    df = df.drop('ACTUALTIME_TRAVEL', 1)\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    # Fitting model based on stops\n",
    "    linear_reg_model = LinearRegression().fit(df, tf)\n",
    "    \n",
    "    # Save to pickle file\n",
    "    with open('/UCD/ResearchPracticum/Data-Analytics-CityRoute/stop_models/stop_'+ stopids_outbound[current_stopid] +'.pkl', 'wb') as handle:\n",
    "        pickle.dump(linear_reg_model, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c12bd",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa9e1c1",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# 5. Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ba17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfm = RandomForestRegressor(n_estimators=40, oob_score=True, random_state=1)\n",
    "rfm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac55d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtc_4 = DecisionTreeRegressor(max_depth=4, random_state=1)\n",
    "dtc_4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e5103",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({'feature': X_train.columns, 'importance': rfm.feature_importances_})\n",
    "importance.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7035a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing the evaluated mode using the hold-out training set\n",
    "rfm_predictions_train = rfm.predict(X_train)\n",
    "predicted_death_yn_Yes = pd.DataFrame(rfm_predictions_train, columns=['Predicted'])\n",
    "actual_vs_predicted_rfm_train = pd.concat([y_train, predicted_death_yn_Yes], axis=1)\n",
    "actual_vs_predicted_rfm_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b85de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(y_train, rfm_predictions_train)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(y_train, rfm_predictions_train))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(y_train, rfm_predictions_train))\n",
    "print(\"R2 Score: \", metrics.r2_score(y_train, rfm_predictions_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ebfae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing the evaluated mode using the hold-out training set\n",
    "rfm_predictions_test = rfm.predict(X_test)\n",
    "predicted_death_yn_test = pd.DataFrame(rfm_predictions_test, columns=['Predicted'])\n",
    "actual_vs_predicted_rfm_test = pd.concat([y_test, predicted_death_yn_test], axis=1)\n",
    "actual_vs_predicted_rfm_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b9edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(y_test, rfm_predictions_test)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(y_test, rfm_predictions_test))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(y_test, rfm_predictions_test))\n",
    "print(\"R2 Score: \", metrics.r2_score(y_test, rfm_predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab7e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "c = int(df['ACTUALTIME_DEP'][500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119265f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "res = int(time.strftime(\"%H\",time.gmtime(c)))\n",
    "print(res >= 7 and res <= 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ca0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
