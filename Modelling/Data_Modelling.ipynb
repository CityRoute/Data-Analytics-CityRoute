{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59cdda17",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "[1. Model Preparation](#1.-Model-Preperation)\n",
    "<br>\n",
    "* [1.1 Reviewing, Splitting data set](#1.1-Reviewing,-splitting-dataset-into-7:3-for-training-and-testing.)\n",
    "* [1.2 Plotting features against target feature](#1.2-Plot-to-compare-all-features-to-target-feature-to-help-make-decisions-to-keep-for-the-models.)\n",
    "    * [1.2.1 Plotting datetime feature against target feature](#Plotting-datetime-feature-against-target-feature)\n",
    "    * [1.2.2 Plotting numerical features against target feature](#Plotting-numerical-features-against-target-feature)\n",
    "    * [1.2.3 Plotting categorical features against target feature](#Plotting-categorical-features-against-target-feature)\n",
    "* [1.3. Summary of all features](#1.3.-Summary-of-all-features)\n",
    "    * [1.3.1 Numerical Features](#Numerical-Features)\n",
    "    * [1.3.1 Cateogrical Features](#Categorical-Features)\n",
    "*[2. Linear Regression & Random Forest & Decision Trees & K-Nearest-Neighbour](#2.-Linear-Regression-&-Random-Forest-&-Decision-Trees-&-K-Nearest-Neighbour)\n",
    "* [3. Route model and taking the proportion of the prediction to calculate a journey time for the user](#3.-Route-model-and-taking-the-proportion-of-the-prediction-to-calculate-a-journey-time-for-the-user.)\n",
    "    * [3.1 Calculating the proportion of each stop from the overall trip](#3.1-Calculating-the-proportion-of-each-stop-from-the-overall-trip.)\n",
    "* [4. Random Forest & Decision Trees](#4.-Random-Forest-&-Decision-Trees)\n",
    "* [5. Stop pair model](#5.-Stop-pair-model)\n",
    "    * [5.1 First version of paired stop approach](#5.1-First-version-of-paired-stop-approach)\n",
    "    * [5.2.1 Setting up for 46a stop pair models using first approach](#5.2.1-Setting-up-for-46a-stop-pair-models-using-first-approach)\n",
    "    * [5.3 Stop pair based on entire leavetimes](#5.3-Stop-pair-based-on-entire-leavetimes)\n",
    "* [6. Final Stop Pair Model](#6.-Final-Stop-Pair-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d23755",
   "metadata": {},
   "source": [
    "Establishing a connection with sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad2ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import sqlite3\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# from sagemaker import et_execution_role\n",
    "from patsy import dmatrices\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from math import log\n",
    "from statistics import stdev\n",
    "from statistics import mode\n",
    "\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Connecting to s3\n",
    "# role = get_execution_role()\n",
    "# bucket='sagemaker-studio-520298385440-7in8n1t299'\n",
    "# data_key = 'route_46a.feather'\n",
    "# data_location = 's3://{}/{}'.format(bucket, data_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a62a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def function to create connection to db\n",
    "def create_connection(db_file):\n",
    "    \"\"\"\n",
    "    create a database connection to the SQLite database specified by db_file\n",
    "    :param df_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try: \n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except 'Error' as e:\n",
    "        print(e)\n",
    "        \n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db01e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create connection to db\n",
    "db_file = \"C:/Users/fayea/UCD/ResearchPracticum/Data-Analytics-CityRoute/dublinbus.db\"\n",
    "conn = create_connection(db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise query\n",
    "query = \"\"\"\n",
    "SELECT leavetimes.*, weather.*\n",
    "FROM leavetimes, weather\n",
    "WHERE TRIPID in  \n",
    "    (SELECT TRIPID\n",
    "    FROM trips\n",
    "    WHERE LINEID = '46A' AND DIRECTION = '1')\n",
    "AND leavetimes.DAYOFSERVICE = weather.dt;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query and read into dataframe\n",
    "query_df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec5b84",
   "metadata": {},
   "source": [
    "# 1. Model Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading file\n",
    "df = query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dfed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather('route46a.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefaac50",
   "metadata": {},
   "source": [
    "## 1.1 Reviewing, splitting dataset into 7:3 for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f523bc15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c18c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837dc6a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unique types for each feature\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57494ad6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Datatypes and convert\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2454b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1882786",
   "metadata": {},
   "source": [
    "**Review so far:**\n",
    "<br>\n",
    "There are no more missing values and the constant columns have been removed.\n",
    "* Remove index, index, dt.\n",
    "* Investigate level_0.\n",
    "* Convert the following to categorical: DAYOFWEEK, MONTHOFSERVICE, PROGRNUMBER, STOPPOINTID, VEHICLEID, IS_HOLIDAY, IS_WEEKDAY, TRIPID, weather_id, weather_main, weather_description\n",
    "* We have data for most of the days of the year and for each month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['level_0', 'dt','index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5377c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by trip then dayofservice\n",
    "df['PROGRNUMBER'] = df['PROGRNUMBER'].astype('int64')\n",
    "df = df.sort_values(by=['TRIPID', 'DAYOFSERVICE', 'PROGRNUMBER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dffdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating features\n",
    "categorical_features = ['DAYOFWEEK', 'MONTHOFSERVICE', 'PROGRNUMBER', 'STOPPOINTID', 'PREVIOUS_STOPPOINTID',\n",
    "                       'IS_HOLIDAY', 'IS_WEEKDAY', 'TRIPID', 'VEHICLEID', 'weather_id', 'weather_main', 'weather_description']\n",
    "\n",
    "datetime_features = ['DAYOFSERVICE']\n",
    "\n",
    "numerical_features = ['PLANNEDTIME_ARR', 'ACTUALTIME_ARR', 'PLANNEDTIME_DEP', 'ACTUALTIME_DEP',\n",
    "                     'DWELLTIME', 'PLANNEDTIME_TRAVEL', 'temp', 'pressure', 'humidity', 'wind_speed', 'wind_deg', 'rain_1h', 'clouds_all']\n",
    "\n",
    "target_feat = 'ACTUALTIME_TRAVEL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a861f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting object to categorical\n",
    "for column in categorical_features:\n",
    "    df[column] = df[column].astype('category')\n",
    "    \n",
    "# Converting dayofservice to datetime\n",
    "df['DAYOFSERVICE'] = pd.to_datetime(df['DAYOFSERVICE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4189963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing PROGRNUMBER equal to 1 of ACTUALTIME_TRAVEL with 0\n",
    "df.loc[df['PROGRNUMBER'] == '1', 'ACTUALTIME_TRAVEL'] = 0\n",
    "df.loc[df['PROGRNUMBER'] == '1', 'PLANNEDTIME_TRAVEL'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d380d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['PLANNEDTIME_TRAVEL'] < 0, 'PLANNEDTIME_TRAVEL'] = 0\n",
    "df.loc[df['ACTUALTIME_TRAVEL'] < 0, 'ACTUALTIME_TRAVEL'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18abbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HOUROFSERVICE'] = [int(time.strftime(\"%H\",time.gmtime(hour))) for hour in df['ACTUALTIME_DEP']]\n",
    "df['eve_rushour'] = [1 if int(time.strftime(\"%H\",time.gmtime(hour))) >= 16 and int(time.strftime(\"%H\",time.gmtime(hour))) <= 19 else 0 for hour in df['ACTUALTIME_DEP']]\n",
    "df['morn_rushour'] = [1 if int(time.strftime(\"%H\",time.gmtime(hour))) >= 7 and int(time.strftime(\"%H\",time.gmtime(hour))) <= 9 else 0 for hour in df['ACTUALTIME_DEP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf43f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df.to_feather('route46a.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d173d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making new feature for previous stoppointid and let those with PROGRNUMBER = 1 to 0\n",
    "# df['PREVIOUS_STOPPOINTID'] = df['STOPPOINTID'].shift()\n",
    "# first_stop = {'0':'0'}\n",
    "# df['PREVIOUS_STOPPOINTID'] = df['PREVIOUS_STOPPOINTID'].cat.add_categories(first_stop)\n",
    "# df.loc[df['PROGRNUMBER'] == '1', 'PREVIOUS_STOPPOINTID'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa66e4",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Setting the target feature as _y and x_ as the remaining features in the dataframe. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(np.random.permutation(df.index))\n",
    "# sort the resulting random index\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating y and x axis\n",
    "target_feature = df['ACTUALTIME_TRAVEL']\n",
    "y = pd.DataFrame(target_feature)\n",
    "X = df.drop(['ACTUALTIME_TRAVEL'], axis=1)\n",
    "\n",
    "# Splitting dataset for train and testing data by 70/30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Printing shape of the new split data\n",
    "print(\"The original range is: \",df.shape[0])\n",
    "print(\"The training range (70%):\\t rows 0 to\", round(X_train.shape[0]))\n",
    "print(\"The test range (30%): \\t rows\", round(X_train.shape[0]), \"to\", round(X_train.shape[0]) + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12306ab",
   "metadata": {},
   "source": [
    "## 1.2 Plot to compare all features to target feature to help make decisions to keep for the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e3e69",
   "metadata": {},
   "source": [
    "#### Plotting datetime feature against target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7390280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot datetime feature against target feature\n",
    "X_train.DAYOFSERVICE = pd.to_numeric(X_train.DAYOFSERVICE)\n",
    "df_temp = pd.concat([X_train['DAYOFSERVICE'], y_train], axis=1)\n",
    "correlation_dt = df_temp[['DAYOFSERVICE', 'ACTUALTIME_TRAVEL']].corr(method='pearson')\n",
    "correlation_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68c32f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('PLOT: DAYOFSERVICE')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot\n",
    "df_temp.plot(kind='scatter', x='DAYOFSERVICE', y='ACTUALTIME_TRAVEL', label = \"%.3f\" % df_temp[['ACTUALTIME_TRAVEL', 'DAYOFSERVICE']].corr().to_numpy()[0,1], figsize=(15, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f23776a",
   "metadata": {},
   "source": [
    "#### Plotting numerical features against target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f930bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in numerical_features:\n",
    "    df_temp = pd.concat([X_train[column], y_train], axis=1)\n",
    "    correlation_dt = df_temp[[column, 'ACTUALTIME_TRAVEL']].corr(method='pearson')\n",
    "    print('\\n',correlation_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e6f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in numerical_features:\n",
    "    df_temp = pd.concat([X_train[column], y_train], axis=1)\n",
    "    correlation_dt = df_temp[[column, 'ACTUALTIME_TRAVEL']].corr(method='spearman')\n",
    "    print('\\n',correlation_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf297b4",
   "metadata": {},
   "source": [
    "#### Pearson correlation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2823d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('NUMERICAL FEATURES: PEARSON')\n",
    "for column in numerical_features:\n",
    "    df_temp = pd.concat([X_train[column], y_train], axis=1)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot\n",
    "    df_temp.plot(kind='scatter', x=column, y='ACTUALTIME_TRAVEL', label = \"%.3f\" % df_temp[['ACTUALTIME_TRAVEL', column]].corr(method='pearson').to_numpy()[0,1], figsize=(12, 8))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56d313",
   "metadata": {},
   "source": [
    "#### Spearman correlation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db9878f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('NUMERICAL FEATURES: SPEARSMAN')\n",
    "for column in numerical_features:\n",
    "    df_temp = pd.concat([X_train[column], y_train], axis=1)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot\n",
    "    df_temp.plot(kind='scatter', x=column, y='ACTUALTIME_TRAVEL', label = \"%.3f\" % df_temp[['ACTUALTIME_TRAVEL', column]].corr(method='spearman').to_numpy()[0,1], figsize=(12, 8))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cffe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NUMERICAL FEATURES: USING CORR()')\n",
    "df.corr()['ACTUALTIME_TRAVEL'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df[numerical_features]\n",
    "for feature in df_numeric:\n",
    "    df_numeric[feature] = np.log(df_numeric[feature])\n",
    "df_numeric['ACTUALTIME_TRAVEL'] = np.log(df['ACTUALTIME_TRAVEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6daf631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('NUMERICAL FEATURES USING LOG DATA')\n",
    "# Creating y and x axis\n",
    "target_feature_numeric = df_numeric['ACTUALTIME_TRAVEL']\n",
    "y_numeric = pd.DataFrame(target_feature_numeric)\n",
    "X_numeric = df_numeric.drop(['ACTUALTIME_TRAVEL'], axis=1)\n",
    "\n",
    "# Splitting dataset for train and testing data by 70/30\n",
    "X_train_numeric, X_test_numeric, y_train_numeric, y_test_numeric = train_test_split(X_numeric, y_numeric, test_size=0.3, random_state=1)\n",
    "\n",
    "# Printing shape of the new split data\n",
    "print(\"The original range is: \",df.shape[0])\n",
    "print(\"The training range (70%):\\t rows 0 to\", round(X_train_numeric.shape[0]))\n",
    "print(\"The test range (30%): \\t rows\", round(X_train_numeric.shape[0]), \"to\", round(X_train_numeric.shape[0]) + X_test_numeric.shape[0])\n",
    "\n",
    "for column in numerical_features:\n",
    "    df_temp = pd.concat([X_train_numeric[column], y_train_numeric], axis=1)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot\n",
    "    df_temp.plot(kind='scatter', x=column, y='ACTUALTIME_TRAVEL', label = \"%.3f\" % df_temp[['ACTUALTIME_TRAVEL', column]].corr(method='spearman').to_numpy()[0,1], figsize=(12, 8))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417625f3",
   "metadata": {},
   "source": [
    "#### Plotting categorical features against target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df27e21d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year_features = ['eve_rushour', 'morn_rushour','DAYOFWEEK', 'IS_HOLIDAY', 'IS_WEEKDAY', 'MONTHOFSERVICE', 'weather_id', 'weather_main', 'weather_description']\n",
    "\n",
    "for feature in year_features:\n",
    "    print(feature)\n",
    "    df_temp = pd.concat([X_train, y_train], axis=1)\n",
    "    unique = df_temp[feature].unique()\n",
    "    list_average = []\n",
    "    \n",
    "    for value in unique:\n",
    "        list_values = df_temp[df_temp[feature]== value]['ACTUALTIME_TRAVEL'].tolist()\n",
    "        length_list = len(list_values)\n",
    "        average =  sum(list_values)/length_list\n",
    "        list_average += [average]\n",
    "#         print(f'Sum of values / list of values: \\n {sum(list_values)} / {length_list}')\n",
    "#         print(f'Average ACTUALTIME_TRAVEL: {average}, \\n')\n",
    "        \n",
    "    # taken from https://pythonspot.com/matplotlib-bar-chart/\n",
    "    y_pos = np.arange(len(unique))\n",
    "    plt.bar(y_pos, list_average, align='center')\n",
    "    plt.xticks(y_pos, unique)\n",
    "    plt.ylabel('Usage')\n",
    "    plt.title(feature)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0401900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average time for each vehicle id\n",
    "df_temp = pd.concat([X_train, y_train], axis=1)\n",
    "vehicleid = df_temp['VEHICLEID'].unique().tolist()\n",
    "for id_ in vehicleid:\n",
    "    print(f'VEHICLEID: {id_}')\n",
    "    list_values = df_temp[df_temp['VEHICLEID']== id_]['ACTUALTIME_TRAVEL'].tolist()\n",
    "    length_list = len(list_values)\n",
    "    average =  sum(list_values)/length_list\n",
    "    print(f'Average ACTUALTIME_TRAVEL: {average} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a36ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dummy variables for categorical \n",
    "cat = ['DAYOFWEEK', 'MONTHOFSERVICE', 'PROGRNUMBER', 'STOPPOINTID', 'IS_HOLIDAY', 'IS_WEEKDAY', 'weather_id', 'weather_main', 'weather_description']\n",
    "df_temp = pd.concat([X_train, y_train], axis=1)\n",
    "df_copy = df_temp.copy()\n",
    "df_copy = df_copy[cat]\n",
    "df_copy = pd.get_dummies(df_copy)\n",
    "df_copy = pd.concat([df_copy, y_train], axis=1)\n",
    "\n",
    "categorical_corr = df_copy.corr()['ACTUALTIME_TRAVEL'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddbe54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(categorical_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a1aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_list = categorical_corr[categorical_corr > 0.04].index.tolist()\n",
    "categorical_list.remove('ACTUALTIME_TRAVEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0462f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categorical_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e0f51",
   "metadata": {},
   "source": [
    "## 1.3. Summary of all features\n",
    "<br><br>\n",
    "#### Numerical Features\n",
    "<br><br>\n",
    "\n",
    "**DayOfService:**\n",
    "* The correlation to the target feature is very low of 0.03806.\n",
    "* Don't see it being a useful feature for the target feature. \n",
    "* Plot represents a straight line, which suggests little to no correlation.\n",
    "* Conclusion: dropped because of the low correlation score. \n",
    "\n",
    "**PlannedTime_Arr:**\n",
    "* There is very low correlation against the target feature though it gets better using spearman correlation.\n",
    "* After logging the data, the correlation plot did not make a huge difference when using the spearman method to plot it for the second time. \n",
    "* Pearson and spearman plot pre log suggests little correlation as it is a continuous straight line. However, this shouldn't mean it should be dropped.\n",
    "* When most values in the target feature fell less than 10, we see that the plannedtime arrival values increasing, it didn't change much. This would be due to the fact that the target feature is the difference between times so it would make sense that the relationship is poor.\n",
    "* After logging the data, the plot is more spread out instead of a straight line, but the correlation score still shows a similar low score with a .02 difference using the spearman method. \n",
    "* Conclusion: However, this will be dropped.\n",
    "\n",
    "**ActualTime_Arr:**\n",
    "* Compared to Planned time arrival feature, the pearson correlation score is poorer but the spearman scores are more similar pre log. \n",
    "* It is similar to planned time arrival in that the plot represents a straight line, that suggests a poor relationship with the target feature. \n",
    "* After logging the data, it is found that the plot is more spread out. The score using spearman is not much different pre logging the data. \n",
    "* However, it would be unwise to drop this feature as it I feel it would serve good purpose for the target feature for predicting the prediction time for the next stop. \n",
    "* Conclusion: this will be dropped.\n",
    "\n",
    "**PlannedTime_Dep:**\n",
    "* Planned time departure has little correlation with the target feature after looking at spearman and pearsons. \n",
    "* It doesn't have a linear relationship and the straight line on the plot of both methods proves this.\n",
    "* However, when plotted using the logged values we see that the correlation score hasn't changed but the data is more spread out. \n",
    "* This doesn't change the relationship much, however. \n",
    "* Even so, this will be kept as I feel it would help the predictions. Having the planned time departures would help skew a better result because it would relatively be close to the actual time departure even though it is just an estimate.\n",
    "* Conclusion: this will be dropped \n",
    "\n",
    "**ActualTime_Dep:**\n",
    "* Actual time departure is again, more or less the same. It represents the departure for these times at a particular stop to go to the next stop. It is strange that the correlation is so low even after logging the data but it would make sense as you wouldn't expect there to be a linear relationship.\n",
    "* The plot is similar to the rest of the previous features mentioned so far. \n",
    "* However, it will still be kept because I feel it would still be a useful feature for predicting a time in seconds. \n",
    "* By taking the actual time departure for a particular stop it may help.\n",
    "* Conclusion: this will be dropped.\n",
    "\n",
    "**Dwell Time:**\n",
    "* Dwell time has a 0.03 coorelation score with the target feature. It suggests on the graph that the time for dwell time equal to 0 then the more the target feature time increases. It might suggest traffic times where if a bus is full then it might be due to rush hour? busy hours?\n",
    "* Plotting against the target feature after logging the data gives similar scores using the spearman correlation method. However we see the graph differing from pre log plot. It is more grouped up together compared to the previous graph plot.\n",
    "* Because the score is more fairer compared to the previous, it will be useful to keep it for the modelling.\n",
    "* Conclusion: dropped.\n",
    "\n",
    "**PlannedTime_Travel:**\n",
    "* When plotting using the pearse correlation method, it gave a correlation of 0.2. This time it is the highest correlation and we see a small linear relationship.\n",
    "* The time for planned time travel, as it increases, so does the target feature. It gives us an indication of that slight linear relationship.\n",
    "* Using spearmans to graph the correlation gave us a 0.7 score which is a good indication that the two features has a linear relationship.\n",
    "* Because of this, this feature will be dropped.\n",
    "\n",
    "**Temp:**\n",
    "* Temp  has a negative 0.009 correlation with the target feature and an even poorer linear relationship at -.002.\n",
    "* This indicates a poor linear/monotonic relationship and it will not serve useful for the model.\n",
    "* The graph plots does not give anymore useful information that would give further evidence that it should be kept.\n",
    "* Conclusion: drop.\n",
    "\n",
    "**Pressure:**\n",
    "* It also has a negative linear relationship with the target feature.\n",
    "* When looking at the graph plots for both spearman and pearsons, it does not give any further insights.\n",
    "* For this reason, this feature will be dropped.\n",
    "\n",
    "**Humidity:**\n",
    "* Humidity does not have a strong relationship with the target feature, be it linear or monotonic.\n",
    "* The reason being the correlation using both methods fell < 0.00. \n",
    "* Unfortunately, the graph does not represent anything useful either.\n",
    "* When looking at the logged data plots however, there is a slight difference however it is not signficant enough that this feature should still be kept as there is no distinct relationship that we can see.\n",
    "* Conclusion: drop.\n",
    "\n",
    "**Windspeed:**\n",
    "* No linear relationship.\n",
    "* Indicates a small monotonic relationship.\n",
    "* This means that as the windspeed value increases, the value of the target feature tends to be higher as well.\n",
    "* But a spearman correlation of 0.01 is not strong enough of a feature to keep.\n",
    "* Conclusion: drop\n",
    "\n",
    "**Wind_Deg:**\n",
    "* This feature will be dropped immediately as the correalations are both <0.000.\n",
    "\n",
    "**Rain_1H:**\n",
    "* It doesn't have a strong linear relationship but it shows spearmans correlation some promising results when the data has been logged.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "#### Categorical Features\n",
    "<br><br>\n",
    "**DayOfWeek:**\n",
    "* In the graph we see the actual time travel increasing during weekdays and slowly the travel time is less during weekends. \n",
    "* This suggests a relationship between the days of the week and the target feature in which weekdays have a higher tendency for the actualtime travel feature to be higher.\n",
    "* Conclusion: this will be kept.\n",
    "\n",
    "**MonthofService:**\n",
    "* In the graph, we don't really see a connection between each month against the target feature even if it is in order. \n",
    "* The overall actual travel time is higher in february before it dips, then rising during winter season.\n",
    "* The correlation score seems to be poor also for each month. \n",
    "* This feature will still be kept. \n",
    "\n",
    "**Progrnumber:**\n",
    "* Most progrnumbers will be dropped as a lot of the correlations are <0.00.\n",
    "* For this reason, this feature will be dropped.\n",
    "    \n",
    "**StoppointID:**\n",
    "* Similarly to progrnumbers, there are a lot of low correlations falling <0.00.\n",
    "* Most stoppoint numbers are <0.00 correlation.\n",
    "* This indicates a very low relationship with the target feature. \n",
    "* For this reason, this feature will be dropped, except for those with a correlation > 0.04\n",
    "    \n",
    "**Is_Holiday:**\n",
    "* After analyzing the graph, we see a relationship between the target feature and whether or not the time falls under a holiday date (non-school holiday).\n",
    "* If it a non holiday, the actual time travel increases. \n",
    "* If it is a holiday, the actual time travel decreases. \n",
    "* This means that less people are using public transport if it is a holiday date.\n",
    "* For this reason, this feature will be kept.\n",
    "\n",
    "**Is_Weekday:**\n",
    "* Like Is_Holiday, we see a relationship between the target feature and whether or not the time is during a weekday or not. \n",
    "* We see a contrast between the two values in which 1, being a weekday, has a higher actual time travel, vice versa.\n",
    "* For this reason, it is a good indication of a relationship to the target feature.\n",
    "* Therefore, this feature will be kept. \n",
    "\n",
    "**VehicleID:**\n",
    "* When looking at the different averages, we see that the average differences are not big.\n",
    "* For this reason, it may be best to drop this feature because it doesn't give any indication it would be a useful feature to help the prediction models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5cfc3e",
   "metadata": {},
   "source": [
    "## 1.4 Cleaning up features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60620df",
   "metadata": {},
   "source": [
    "### Setting low correlation features - keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c417f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features \n",
    "low_corr_categorical = ['DAYOFWEEK', 'MONTHOFSERVICE', 'IS_HOLIDAY', 'IS_WEEKDAY'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cb7079",
   "metadata": {},
   "source": [
    "### Setting low correlation features - drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features\n",
    "low_corr_numerical = ['PLANNEDTIME_ARR', 'PLANNEDTIME_DEP', 'ACTUALTIME_ARR', 'ACTUALTIME_DEP','PLANNEDTIME_TRAVEL']\n",
    "\n",
    "low_corr = ['DAYOFSERVICE', 'VEHICLEID', 'TRIPID', 'STOPPOINTID', 'PREVIOUS_STOPPOINTID', 'PROGRNUMBER', 'temp', 'pressure', 'humidity', \n",
    "            'wind_deg', 'weather_id', 'weather_description', 'clouds_all', 'wind_speed', 'PREVIOUS_STOPPOINTID', 'PLANNEDTIME_ARR', 'PLANNEDTIME_DEP', 'ACTUALTIME_ARR', 'ACTUALTIME_DEP',\n",
    "           'PLANNEDTIME_TRAVEL', 'DWELLTIME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3a89ee",
   "metadata": {},
   "source": [
    "### Setting high correlation  features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e969a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features \n",
    "high_corr_numerical = ['DWELLTIME', 'PLANNEDTIME_TRAVEL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d2ffa",
   "metadata": {},
   "source": [
    "### Dropping features & setting dummy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ee73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_copy = df_copy.drop(low_corr, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df258c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_copy = pd.get_dummies(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2fab3f",
   "metadata": {},
   "source": [
    "### Training & Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a18a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# All features\n",
    "features = df_copy.columns.tolist()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c133c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = {'ACTUALTIME_TRAVEL': df_copy['ACTUALTIME_TRAVEL']}\n",
    "y = pd.DataFrame(data=datas)\n",
    "X = df_copy.drop(['ACTUALTIME_TRAVEL'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe715dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into 2 datasets: \n",
    "# Split the dataset into two datasets: 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)\n",
    "\n",
    "print(\"The Original range of the dataset: \",df.shape[0])\n",
    "print(\"The Training range taken from dataset: (70%): rows 0 to\", round(X_train.shape[0]))\n",
    "print(\"The Test range taken from dataset: (30%): rows\", round(X_train.shape[0]), \"to\", round(X_train.shape[0]) + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7780b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nDescriptive features in X:\\n\", X_train.head(5))\n",
    "print(\"\\nTarget feature in y:\\n\", y_train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e924e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will reset the indexes of the training and test splits so we can see the X_train printout\n",
    "# We will see that they are no longer in order and the next markdown cell I will reset the indexes.\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5ce811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .reset_index \n",
    "# We see that they are in order again. \n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad45e8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643efa4",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# 2. Linear Regression & Random Forest & Decision Trees & K Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationMetrics:\n",
    "    \n",
    "    def __init__(self, dataframe, train_route):\n",
    "        self.dataframe = dataframe\n",
    "        self.train_route = train_route\n",
    "        self.list_stops = self.train_route.STOPPOINTID.unique().tolist()\n",
    "        \n",
    "        self.linear_model = {}\n",
    "        self.rf_model = {}\n",
    "        self.dt_model = {}\n",
    "        self.knn_model = {}\n",
    "        \n",
    "    def training_models(self):\n",
    "        \n",
    "        for previous, current in zip(self.list_stops, self.list_stops[1:]):\n",
    "    \n",
    "            df_stopid = self.dataframe[(self.dataframe['STOPPOINTID']==current) & (self.dataframe['PREVIOUS_STOPPOINTID']==previous)]\n",
    "            df_stopid = df_stopid.drop(low_corr, 1)\n",
    "            df_stopid = pd.get_dummies(df_stopid)\n",
    "            y = pd.DataFrame(df_stopid['ACTUALTIME_TRAVEL'])\n",
    "            df_stopid = df_stopid.drop('ACTUALTIME_TRAVEL', 1)\n",
    "\n",
    "            rfm = RandomForestRegressor(n_estimators=40, oob_score=True, random_state=1)\n",
    "            dtc_4 = DecisionTreeRegressor(max_depth=4, random_state=1)\n",
    "            knn = KNeighborsRegressor()\n",
    "\n",
    "            # Training models\n",
    "            linear_model = LinearRegression().fit(df_stopid, y)\n",
    "            rf_model = rfm.fit(df_stopid, y)\n",
    "            dt_model = dtc_4.fit(df_stopid, y)\n",
    "            knn_model = knn.fit(df_stopid, y)\n",
    "\n",
    "            # Storing models in dictionary\n",
    "            self.linear_model[current + '_' + previous] = linear_model\n",
    "            self.rf_model[current + '_' + previous] = rf_model\n",
    "            self.dt_model[current + '_' + previous] = dt_model\n",
    "            self.knn_model[current + '_' + previous] = knn_model\n",
    "        \n",
    "        print('Models trained!')\n",
    "        \n",
    "        \n",
    "    def make_predictions(self, to_predict):\n",
    "        self.dataframe = to_predict\n",
    "        \n",
    "        # Setting up list for predictions\n",
    "        self.linear_pred = np.zeros(shape=(self.dataframe.shape[1],1))\n",
    "        self.rf_model_pred = np.zeros(shape=(self.dataframe.shape[1],1))\n",
    "        self.dt_model_pred = np.zeros(shape=(self.dataframe.shape[1],1))\n",
    "        self.knn_model_pred = np.zeros(shape=(self.dataframe.shape[1],1))\n",
    "        \n",
    "        predictions_1 = []\n",
    "        predictions_2 = []\n",
    "        predictions_3 = []\n",
    "        predictions_4 = []\n",
    "        index = 0\n",
    "        \n",
    "        for previous, current in zip(self.list_stops, self.list_stops[1:]):\n",
    "            if previous == '807' and current == '817':\n",
    "                continue\n",
    "\n",
    "            predictions_1 += [self.linear_model[current + '_' + previous].predict(self.dataframe.iloc[[index]])]\n",
    "            predictions_2 += [self.linear_model[current + '_' + previous].predict(self.dataframe.iloc[[index]])]\n",
    "            predictions_3 += [self.linear_model[current + '_' + previous].predict(self.dataframe.iloc[[index]])]\n",
    "            predictions_4 += [self.linear_model[current + '_' + previous].predict(self.dataframe.iloc[[index]])]\n",
    "            index += 1\n",
    "\n",
    "        for pred in range(len(31)):\n",
    "            self.linear_pred[pred] = predictions_1[pred][0][0]\n",
    "            self.rf_model_pred[pred] = predictions_2[pred][0]\n",
    "            self.dt_model_pred[pred] = predictions_3[pred][0]\n",
    "            self.knn_model_pred[pred] = predictions_4[pred][0]\n",
    "\n",
    "            \n",
    "        self.master_prediction_list = [self.linear_pred, self.rf_model_pred, self.dt_model_pred, self.knn_model_pred]\n",
    "            \n",
    "        return self.master_prediction_list\n",
    "    \n",
    "    \n",
    "    def get_evalmetrics(self, prediction_list, actual_predictions):\n",
    "        self.prediction_list = prediction_list\n",
    "        self.actual_predictions = actual_predictions\n",
    "        \n",
    "        for model in self.prediction_list:\n",
    "            print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(self.actual_predictions, model)))\n",
    "            print(\"MSE Score: \", metrics.mean_squared_error(self.actual_predictions, model))\n",
    "            print(\"MAE Score: \", metrics.mean_absolute_error(self.actual_predictions, model))\n",
    "            print(\"R2 Score: \", metrics.r2_score(self.actual_predictions, model))\n",
    "\n",
    "            actual_total_linear = sum(self.actual_predictions.ACTUALTIME_TRAVEL)\n",
    "            predicted_total_linear = sum(model)\n",
    "            print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "            print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a58f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "routesample_1 = df[(df['TRIPID'] == '8591174') & (df['DAYOFSERVICE']=='2018-12-23')]\n",
    "sample = routesample_1\n",
    "routesample_1 = routesample_1.drop(low_corr, 1)\n",
    "routesample_1 = pd.get_dummies(routesample_1)\n",
    "actual_routesample_1 = pd.DataFrame(routesample_1['ACTUALTIME_TRAVEL'])\n",
    "routesample_1 = routesample_1.drop('ACTUALTIME_TRAVEL', 1)\n",
    "\n",
    "\n",
    "test = EvaluationMetrics(df, sample)\n",
    "trained_models = test.training_models()\n",
    "predictions = test.make_predictions(routesample_1)\n",
    "test.get_evalmetrics(actual_routesample, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9476e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up route samples\n",
    "routesample_1 = df[(df['TRIPID'] == '8591174') & (df['DAYOFSERVICE']=='2018-12-23')]\n",
    "routesample_2 = df[(df['TRIPID'] == '6106738') & (df['DAYOFSERVICE']==' 2018-01-19')]\n",
    "\n",
    "# List of stops for route 46a\n",
    "stops_46a = routesample_1.STOPPOINTID.tolist()\n",
    "\n",
    "# Setting up dummy features \n",
    "routesample_1 = routesample_1.drop(low_corr, 1)\n",
    "routesample_1 = pd.get_dummies(routesample_1)\n",
    "actual_routesample_1 = pd.DataFrame(routesample_1['ACTUALTIME_TRAVEL'])\n",
    "routesample_1 = routesample_1.drop('ACTUALTIME_TRAVEL', 1)\n",
    "\n",
    "routesample_2 = routesample_2.drop(low_corr, 1)\n",
    "routesample_2 = pd.get_dummies(routesample_2)\n",
    "actual_routesample_2 = pd.DataFrame(routesample_2['ACTUALTIME_TRAVEL'])\n",
    "routesample_2 = routesample_2.drop('ACTUALTIME_TRAVEL', 1)\n",
    "\n",
    "# Setting up models for each model - two versions of training models\n",
    "linear_model_v1 = {}\n",
    "rf_model_v1 = {}\n",
    "dt_model_v1 = {}\n",
    "knn_model_v1 = {}\n",
    "\n",
    "# Setting up list for predictions\n",
    "linear_v1_pred = np.zeros(shape=(59,1))\n",
    "linear_v2_pred = np.zeros(shape=(59,1))\n",
    "rf_model_v1_pred = np.zeros(shape=(59,1))\n",
    "rf_model_v2_pred = np.zeros(shape=(59,1))\n",
    "dt_model_v1_pred = np.zeros(shape=(59,1))\n",
    "dt_model_v2_pred = np.zeros(shape=(59,1))\n",
    "knn_model_v1_pred = np.zeros(shape=(59,1))\n",
    "knn_model_v2_pred = np.zeros(shape=(59,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fba87a",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 2.1 Training without additional features - current stopid and previous stopid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0daa089",
   "metadata": {},
   "outputs": [],
   "source": [
    "for previous, current in zip(stops_46a, stops_46a[1:]):\n",
    "    \n",
    "    df_stopid = df[(df['STOPPOINTID']==current) & (df['PREVIOUS_STOPPOINTID']==previous)]\n",
    "    df_stopid = df_stopid.drop(low_corr, 1)\n",
    "    df_stopid = pd.get_dummies(df_stopid)\n",
    "    y = pd.DataFrame(df_stopid['ACTUALTIME_TRAVEL'])\n",
    "    df_stopid = df_stopid.drop('ACTUALTIME_TRAVEL', 1)\n",
    "        \n",
    "    rfm = RandomForestRegressor(n_estimators=40, oob_score=True, random_state=1)\n",
    "    dtc_4 = DecisionTreeRegressor(max_depth=4, random_state=1)\n",
    "    knn = KNeighborsRegressor()\n",
    "    \n",
    "    # Training models\n",
    "    linear_model = LinearRegression().fit(df_stopid, y)\n",
    "    rf_model = rfm.fit(df_stopid, y)\n",
    "    dt_model = dtc_4.fit(df_stopid, y)\n",
    "    knn_model = knn.fit(df_stopid, y)\n",
    "    \n",
    "    # Storing models in dictionary\n",
    "    linear_model_v1[current + '_' + previous] = linear_model\n",
    "    rf_model_v1[current + '_' + previous] = rf_model\n",
    "    dt_model_v1[current + '_' + previous] = dt_model\n",
    "    knn_model_v1[current + '_' + previous] = knn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38212c08",
   "metadata": {},
   "source": [
    "### 2.1.1 Obtaining predictions - route sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17685d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "predictions_1 = []\n",
    "predictions_2 = []\n",
    "predictions_3 = []\n",
    "predictions_4 = []\n",
    "\n",
    "for previous, current in zip(stops_46a, stops_46a[1:]):\n",
    "    if previous == '807' and current == '817':\n",
    "        continue\n",
    "    \n",
    "    predictions_1 += [linear_model_v1[current + '_' + previous].predict(routesample_1.iloc[[index]])]\n",
    "    predictions_2 += [rf_model_v1[current + '_' + previous].predict(routesample_1.iloc[[index]])]\n",
    "    predictions_3 += [dt_model_v1[current + '_' + previous].predict(routesample_1.iloc[[index]])]\n",
    "    predictions_4 += [knn_model_v1[current + '_' + previous].predict(routesample_1.iloc[[index]])]\n",
    "    index += 1\n",
    "\n",
    "predictions_2[0][0]\n",
    "for pred in range(len(predictions_1)):\n",
    "    linear_v1_pred[pred] = predictions_1[pred][0][0]\n",
    "    rf_model_v1_pred[pred] = predictions_2[pred][0]\n",
    "    dt_model_v1_pred[pred] = predictions_3[pred][0]\n",
    "    knn_model_v1_pred[pred] = predictions_4[pred][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a9aaf",
   "metadata": {},
   "source": [
    "### 2.1.2 Obtaining predictions - route sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "predictions_1 = []\n",
    "predictions_2 = []\n",
    "predictions_3 = []\n",
    "predictions_4 = []\n",
    "\n",
    "for previous, current in zip(stops_46a, stops_46a[1:]):\n",
    "    if previous == '807' and current == '817':\n",
    "        continue\n",
    "    \n",
    "    predictions_1 += [linear_model_v1[current + '_' + previous].predict(routesample_2.iloc[[index]])]\n",
    "    predictions_2 += [rf_model_v1[current + '_' + previous].predict(routesample_2.iloc[[index]])]\n",
    "    predictions_3 += [dt_model_v1[current + '_' + previous].predict(routesample_2.iloc[[index]])]\n",
    "    predictions_4 += [knn_model_v1[current + '_' + previous].predict(routesample_2.iloc[[index]])]\n",
    "    index += 1\n",
    "    \n",
    "\n",
    "for pred in range(len(predictions_1)):\n",
    "    linear_v2_pred[pred] = predictions_1[pred][0][0]\n",
    "    rf_model_v2_pred[pred] = predictions_2[pred][0]\n",
    "    dt_model_v2_pred[pred] = predictions_3[pred][0]\n",
    "    knn_model_v2_pred[pred] = predictions_4[pred][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3790f0",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Printing evaluation metrics for route sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print('Linear Model Evaluation Metrics: \\n')\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actual_routesample_1, linear_v1_pred)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actual_routesample_1, linear_v1_pred))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actual_routesample_1, linear_v1_pred))\n",
    "print(\"R2 Score: \", metrics.r2_score(actual_routesample_1, linear_v1_pred))\n",
    "\n",
    "actual_total_linear = sum(actual_routesample_1.ACTUALTIME_TRAVEL)\n",
    "predicted_total_linear = sum(linear_v1_pred)\n",
    "print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc10f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print('Random Forest Evaluation Metrics: \\n')\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actual_routesample_1, rf_model_v1_pred)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actual_routesample_1, rf_model_v1_pred))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actual_routesample_1, rf_model_v1_pred))\n",
    "print(\"R2 Score: \", metrics.r2_score(actual_routesample_1, rf_model_v1_pred))\n",
    "\n",
    "actual_total_linear = sum(actual_routesample_1.ACTUALTIME_TRAVEL)\n",
    "predicted_total_linear = sum(rf_model_v1_pred)\n",
    "print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4286c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print('Decision Trees Evaluation Metrics: \\n')\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actual_routesample_1, dt_model_v1_pred)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actual_routesample_1, dt_model_v1_pred))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actual_routesample_1, dt_model_v1_pred))\n",
    "print(\"R2 Score: \", metrics.r2_score(actual_routesample_1, dt_model_v1_pred))\n",
    "\n",
    "actual_total_linear = sum(actual_routesample_1.ACTUALTIME_TRAVEL)\n",
    "predicted_total_linear = sum(dt_model_v1_pred)\n",
    "print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a332a803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print('KNN Evaluation Metrics: \\n')\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actual_routesample_1, knn_model_v1_pred)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actual_routesample_1, knn_model_v1_pred))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actual_routesample_1, knn_model_v1_pred))\n",
    "print(\"R2 Score: \", metrics.r2_score(actual_routesample_1, knn_model_v1_pred))\n",
    "\n",
    "actual_total_linear = sum(actual_routesample_1.ACTUALTIME_TRAVEL)\n",
    "predicted_total_linear = sum(knn_model_v1_pred)\n",
    "print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bb79a2",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Printing evaluation metrics for route sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print('Linear Model Evaluation Metrics: \\n')\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actual_routesample_2, linear_v2_pred)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actual_routesample_2, linear_v2_pred))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actual_routesample_2, linear_v2_pred))\n",
    "print(\"R2 Score: \", metrics.r2_score(actual_routesample_2, linear_v2_pred))\n",
    "\n",
    "actual_total_linear = sum(actual_routesample_1.ACTUALTIME_TRAVEL)\n",
    "predicted_total_linear = sum(linear_v2_pred)\n",
    "print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fcfd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print('Random Forest Model Evaluation Metrics: \\n')\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actual_routesample_2, rf_model_v2_pred)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actual_routesample_2, rf_model_v2_pred))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actual_routesample_2, rf_model_v2_pred))\n",
    "print(\"R2 Score: \", metrics.r2_score(actual_routesample_2, rf_model_v2_pred))\n",
    "\n",
    "actual_total_linear = sum(actual_routesample_1.ACTUALTIME_TRAVEL)\n",
    "predicted_total_linear = sum(rf_model_v2_pred)\n",
    "print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print('Decition Tree Model Evaluation Metrics: \\n')\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actual_routesample_2, dt_model_v2_pred)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actual_routesample_2, dt_model_v2_pred))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actual_routesample_2, dt_model_v2_pred))\n",
    "print(\"R2 Score: \", metrics.r2_score(actual_routesample_2, dt_model_v2_pred))\n",
    "\n",
    "actual_total_linear = sum(actual_routesample_1.ACTUALTIME_TRAVEL)\n",
    "predicted_total_linear = sum(dt_model_v2_pred)\n",
    "print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d03e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print('KNN Model Evaluation Metrics: \\n')\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actual_routesample_2, knn_model_v2_pred)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actual_routesample_2, knn_model_v2_pred))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actual_routesample_2, knn_model_v2_pred))\n",
    "print(\"R2 Score: \", metrics.r2_score(actual_routesample_2, knn_model_v2_pred))\n",
    "\n",
    "actual_total_linear = sum(actual_routesample_1.ACTUALTIME_TRAVEL)\n",
    "predicted_total_linear = sum(knn_model_v2_pred)\n",
    "print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bda691d",
   "metadata": {},
   "source": [
    "## 2.2 Training with additional features  - current stopid and previous stopid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaca2cc",
   "metadata": {},
   "source": [
    "[Back to top section](#2.1-Training-without-additional-features---current-stopid-and-previous-stopid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d6c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file = open('stop_dwelltimes.json',)\n",
    "stop_dwelltimes = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8407f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_corr = ['DAYOFSERVICE', 'VEHICLEID', 'TRIPID', 'STOPPOINTID', 'PREVIOUS_STOPPOINTID', 'PROGRNUMBER', 'temp', 'pressure', 'humidity', \n",
    "            'wind_deg', 'weather_id', 'weather_description', 'clouds_all', 'wind_speed', 'PREVIOUS_STOPPOINTID', 'PLANNEDTIME_ARR', 'PLANNEDTIME_DEP', 'ACTUALTIME_ARR', 'ACTUALTIME_DEP',\n",
    "           'PLANNEDTIME_TRAVEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6864a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making new features\n",
    "# df['HOUROFSERVICE'] = [int(time.strftime(\"%H\",time.gmtime(hour))) for hour in df['ACTUALTIME_DEP']]\n",
    "df['eve_rushour'] = [1 if int(time.strftime(\"%H\",time.gmtime(hour))) >= 16 and int(time.strftime(\"%H\",time.gmtime(hour))) <= 19 else 0 for hour in df['ACTUALTIME_DEP']]\n",
    "df['morn_rushour'] = [1 if int(time.strftime(\"%H\",time.gmtime(hour))) >= 7 and int(time.strftime(\"%H\",time.gmtime(hour))) <= 9 else 0 for hour in df['ACTUALTIME_DEP']]\n",
    "df['morn_rushour'] = df['morn_rushour'].astype('category')\n",
    "df['eve_rushour'] = df['eve_rushour'].astype('category')\n",
    "# df = df.drop('HOUROFSERVICE',  1)\n",
    "# df = df.drop('morn_rushour',  1)\n",
    "\n",
    "\n",
    "# Setting up route samples\n",
    "routesample_1 = df[(df['TRIPID'] == '8591174') & (df['DAYOFSERVICE']=='2018-12-23')]\n",
    "routesample_2 = df[(df['TRIPID'] == '6106738') & (df['DAYOFSERVICE']==' 2018-01-19')]\n",
    "\n",
    "# List of stops for route 46a\n",
    "stops_46a = routesample_1.STOPPOINTID.tolist()\n",
    "\n",
    "# Setting up dummy features ]\n",
    "routesample_1 = routesample_1.drop(low_corr, 1)\n",
    "routesample_1 = pd.get_dummies(routesample_1)\n",
    "actual_routesample_1 = pd.DataFrame(routesample_1['ACTUALTIME_TRAVEL'])\n",
    "routesample_1 = routesample_1.drop('ACTUALTIME_TRAVEL', 1)\n",
    "\n",
    "routesample_2 = routesample_2.drop(low_corr, 1)\n",
    "routesample_2 = pd.get_dummies(routesample_2)\n",
    "actual_routesample_2 = pd.DataFrame(routesample_2['ACTUALTIME_TRAVEL'])\n",
    "routesample_2 = routesample_2.drop('ACTUALTIME_TRAVEL', 1)\n",
    "\n",
    "# Setting up dictionary to store trained models\n",
    "linear_model_v2 = {}\n",
    "dt_model_v2 = {}\n",
    "rf_model_v2 = {}\n",
    "knn_model_v2 = {}\n",
    "\n",
    "# Setting up empty arrays to feed predictions into it\n",
    "linear_v1_pred = np.zeros(shape=(59,1))\n",
    "linear_v2_pred = np.zeros(shape=(59,1))\n",
    "rf_model_v1_pred = np.zeros(shape=(59,1))\n",
    "rf_model_v2_pred = np.zeros(shape=(59,1))\n",
    "dt_model_v1_pred = np.zeros(shape=(59,1))\n",
    "dt_model_v2_pred = np.zeros(shape=(59,1))\n",
    "knn_model_v1_pred = np.zeros(shape=(59,1))\n",
    "knn_model_v2_pred = np.zeros(shape=(59,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7981159",
   "metadata": {},
   "outputs": [],
   "source": [
    "for previous, current in zip(stops_46a, stops_46a[1:]):\n",
    "    \n",
    "    df_stopid = df[(df['STOPPOINTID']==current) & (df['PREVIOUS_STOPPOINTID']==previous)]\n",
    "    df_stopid = df_stopid.drop(low_corr, 1)\n",
    "    df_stopid = pd.get_dummies(df_stopid)\n",
    "    y = pd.DataFrame(df_stopid['ACTUALTIME_TRAVEL'])\n",
    "    df_stopid = df_stopid.drop('ACTUALTIME_TRAVEL', 1)\n",
    "        \n",
    "    rfm = RandomForestRegressor(n_estimators=40, oob_score=True, random_state=1)\n",
    "    dtc_4 = DecisionTreeRegressor(max_depth=4, random_state=1)\n",
    "    knn = KNeighborsRegressor()\n",
    "    \n",
    "    # Training models\n",
    "    linear_model = LinearRegression().fit(df_stopid, y)\n",
    "    rf_model = rfm.fit(df_stopid, y)\n",
    "    dt_model = dtc_4.fit(df_stopid, y)\n",
    "    knn_model = knn.fit(df_stopid, y)\n",
    "    \n",
    "    # Storing models in dictionary\n",
    "    linear_model_v2[current + '_' + previous] = linear_model\n",
    "    rf_model_v2[current + '_' + previous] = rf_model\n",
    "    dt_model_v2[current + '_' + previous] = dt_model\n",
    "    knn_model_v2[current + '_' + previous] = knn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c412faa",
   "metadata": {},
   "source": [
    "### 2.2.1 Obtaining predictions - route sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9509a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "predictions_1 = []\n",
    "predictions_2 = []\n",
    "predictions_3 = []\n",
    "predictions_4 = []\n",
    "\n",
    "for previous, current in zip(stops_46a, stops_46a[1:]):\n",
    "    if previous == '807' and current == '817':\n",
    "        continue\n",
    "    \n",
    "    predictions_1 += [linear_model_v2[current + '_' + previous].predict(routesample_1.iloc[[index]])]\n",
    "    predictions_2 += [rf_model_v2[current + '_' + previous].predict(routesample_1.iloc[[index]])]\n",
    "    predictions_3 += [dt_model_v2[current + '_' + previous].predict(routesample_1.iloc[[index]])]\n",
    "    predictions_4 += [knn_model_v2[current + '_' + previous].predict(routesample_1.iloc[[index]])]\n",
    "    index += 1\n",
    "\n",
    "for pred in range(len(predictions_1)):\n",
    "    linear_v1_pred[pred] = predictions_1[pred][0][0]\n",
    "    rf_model_v1_pred[pred] = predictions_2[pred][0]\n",
    "    dt_model_v1_pred[pred] = predictions_3[pred][0]\n",
    "    knn_model_v1_pred[pred] = predictions_4[pred][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37699c46",
   "metadata": {},
   "source": [
    "### 2.2.2 Obtaining predictions - route sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd7b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "predictions_1 = []\n",
    "predictions_2 = []\n",
    "predictions_3 = []\n",
    "predictions_4 = []\n",
    "\n",
    "for previous, current in zip(stops_46a, stops_46a[1:]):\n",
    "    if previous == '807' and current == '817':\n",
    "        continue\n",
    "    \n",
    "    predictions_1 += [linear_model_v2[current + '_' + previous].predict(routesample_2.iloc[[index]])]\n",
    "    predictions_2 += [rf_model_v2[current + '_' + previous].predict(routesample_2.iloc[[index]])]\n",
    "    predictions_3 += [dt_model_v2[current + '_' + previous].predict(routesample_2.iloc[[index]])]\n",
    "    predictions_4 += [knn_model_v2[current + '_' + previous].predict(routesample_2.iloc[[index]])]\n",
    "    index += 1\n",
    "    \n",
    "\n",
    "for pred in range(len(predictions_1)):\n",
    "    linear_v2_pred[pred] = predictions_1[pred][0][0]\n",
    "    rf_model_v2_pred[pred] = predictions_2[pred][0]\n",
    "    dt_model_v2_pred[pred] = predictions_3[pred][0]\n",
    "    knn_model_v2_pred[pred] = predictions_4[pred][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba1776d",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Printing evaluation metrics for route sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1366938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print('Linear Model Evaluation Metrics: \\n')\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actual_routesample_1, linear_v1_pred)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actual_routesample_1, linear_v1_pred))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actual_routesample_1, linear_v1_pred))\n",
    "print(\"R2 Score: \", metrics.r2_score(actual_routesample_1, linear_v1_pred))\n",
    "\n",
    "actual_total_linear = sum(actual_routesample_1.ACTUALTIME_TRAVEL)\n",
    "predicted_total_linear = sum(linear_v1_pred)\n",
    "print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91a294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print('Random Forest Evaluation Metrics: \\n')\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actual_routesample_1, rf_model_v1_pred)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actual_routesample_1, rf_model_v1_pred))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actual_routesample_1, rf_model_v1_pred))\n",
    "print(\"R2 Score: \", metrics.r2_score(actual_routesample_1, rf_model_v1_pred))\n",
    "\n",
    "actual_total_linear = sum(actual_routesample_1.ACTUALTIME_TRAVEL)\n",
    "predicted_total_linear = sum(rf_model_v1_pred)\n",
    "print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf15893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print('Decision Trees Evaluation Metrics: \\n')\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actual_routesample_1, dt_model_v1_pred)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actual_routesample_1, dt_model_v1_pred))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actual_routesample_1, dt_model_v1_pred))\n",
    "print(\"R2 Score: \", metrics.r2_score(actual_routesample_1, dt_model_v1_pred))\n",
    "\n",
    "actual_total_linear = sum(actual_routesample_1.ACTUALTIME_TRAVEL)\n",
    "predicted_total_linear = sum(dt_model_v1_pred)\n",
    "print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4935ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print('KNN Evaluation Metrics: \\n')\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actual_routesample_1, knn_model_v1_pred)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actual_routesample_1, knn_model_v1_pred))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actual_routesample_1, knn_model_v1_pred))\n",
    "print(\"R2 Score: \", metrics.r2_score(actual_routesample_1, knn_model_v1_pred))\n",
    "\n",
    "actual_total_linear = sum(actual_routesample_1.ACTUALTIME_TRAVEL)\n",
    "predicted_total_linear = sum(knn_model_v1_pred)\n",
    "print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffd56d3",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Printing evaluation metrics for route sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dbf84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print('Linear Model Evaluation Metrics: \\n')\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actual_routesample_2, linear_v2_pred)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actual_routesample_2, linear_v2_pred))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actual_routesample_2, linear_v2_pred))\n",
    "print(\"R2 Score: \", metrics.r2_score(actual_routesample_2, linear_v2_pred))\n",
    "\n",
    "actual_total_linear = sum(actual_routesample_2.ACTUALTIME_TRAVEL)\n",
    "predicted_total_linear = sum(linear_v2_pred)\n",
    "print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print('Random Forest Model Evaluation Metrics: \\n')\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actual_routesample_2, rf_model_v2_pred)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actual_routesample_2, rf_model_v2_pred))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actual_routesample_2, rf_model_v2_pred))\n",
    "print(\"R2 Score: \", metrics.r2_score(actual_routesample_2, rf_model_v2_pred))\n",
    "\n",
    "actual_total_linear = sum(actual_routesample_1.ACTUALTIME_TRAVEL)\n",
    "predicted_total_linear = sum(rf_model_v2_pred)\n",
    "print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ddc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print('Decition Tree Model Evaluation Metrics: \\n')\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actual_routesample_2, dt_model_v2_pred)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actual_routesample_2, dt_model_v2_pred))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actual_routesample_2, dt_model_v2_pred))\n",
    "print(\"R2 Score: \", metrics.r2_score(actual_routesample_2, dt_model_v2_pred))\n",
    "\n",
    "actual_total_linear = sum(actual_routesample_1.ACTUALTIME_TRAVEL)\n",
    "predicted_total_linear = sum(dt_model_v2_pred)\n",
    "print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab2b03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print('KNN Model Evaluation Metrics: \\n')\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actual_routesample_2, knn_model_v2_pred)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actual_routesample_2, knn_model_v2_pred))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actual_routesample_2, knn_model_v2_pred))\n",
    "print(\"R2 Score: \", metrics.r2_score(actual_routesample_2, knn_model_v2_pred))\n",
    "\n",
    "actual_total_linear = sum(actual_routesample_1.ACTUALTIME_TRAVEL)\n",
    "predicted_total_linear = sum(knn_model_v2_pred)\n",
    "print(f'\\nActual total journney time: {actual_total_linear} seconds.')\n",
    "print(f'Predicted total journey time: {predicted_total_linear[0]} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da2a1e3",
   "metadata": {},
   "source": [
    "***\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa91a8",
   "metadata": {},
   "source": [
    "# 3. Route model and taking the proportion of the prediction to calculate a journey time for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa8cd2",
   "metadata": {},
   "source": [
    "## 3.1 Calculating the proportion of each stop from the overall trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb07bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_stops(predictions):\n",
    "    # Sum from the first stop until each stop\n",
    "    sum_each_stop = np.zeros(predictions.shape[0], dtype=float)\n",
    "    proportion_each_stop = np.zeros(predictions.shape[0], dtype=float)\n",
    "    overall_prediction = np.sum(predictions)\n",
    "    \n",
    "    # Adding sum up until current stop and dividing by overall prediction to get proportion of the trip\n",
    "    for length in range(predictions.shape[0]):\n",
    "        sum_each_stop = np.append(sum_each_stop, [predictions[length]])\n",
    "        sum_overall = np.sum(sum_each_stop) / overall_prediction*100\n",
    "        proportion_each_stop[length] = sum_overall\n",
    "        \n",
    "    return proportion_each_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9687e1",
   "metadata": {},
   "source": [
    "## 3.2 Return the progrnumber based off the stoppointid in a route"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbcb809",
   "metadata": {},
   "source": [
    "Finding the most common progrnumber based off the stoppointid. The reason for using to find the most common progrnumber is because it assumes that most route_id for each line would be always complete with the exception of a few trips in which they take a different route and skips some stops as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e8cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from https://www.geeksforgeeks.org/python-find-most-frequent-element-in-a-list/\n",
    "\n",
    "# array only accepts a panda Series or numpy array\n",
    "def most_common(array):\n",
    "    List = array.tolist()\n",
    "    mode_list = mode(List)\n",
    "    if mode_list == '1':\n",
    "        return 0\n",
    "    \n",
    "    else:\n",
    "        return(mode(List))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da87ce03",
   "metadata": {},
   "source": [
    "## 3.3 Calculating the journey time from a start to end destination based on user input\n",
    "\n",
    "Finding the travel time duration based on a stoppointid then getting the progrnumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e56f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def journey_time(start,end, prediction):\n",
    "    # Converting into int because the function returns a string\n",
    "    start_progrnum = int(most_common(df['PROGRNUMBER'][df['STOPPOINTID']==start]))\n",
    "    end_progrnum = int(most_common(df['PROGRNUMBER'][df['STOPPOINTID']==end]))\n",
    "    \n",
    "#     print(start_progrnum)\n",
    "#     print(end_progrnum)\n",
    "\n",
    "    proportion_array = proportion_stops(prediction)\n",
    "    overall_prediction = np.sum(prediction)\n",
    "    \n",
    "    # calculating the time difference from start to end destination \n",
    "    start_prediction = (proportion_array[start_progrnum]/100) * overall_prediction\n",
    "    end_prediction = (proportion_array[end_progrnum]/100) * overall_prediction\n",
    "    \n",
    "    journeytime = end_prediction - start_prediction\n",
    "    \n",
    "    # print(journeytime)\n",
    "    \n",
    "    return journeytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84041e62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_start = '807'\n",
    "user_end = '812'\n",
    "\n",
    "journey_time(user_start, user_end, prediction_46a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c799d97a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824c9ccb",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# 5. Stop pair model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7487941",
   "metadata": {},
   "source": [
    "## 5.1 First version of paired stop approach\n",
    "<br><br>\n",
    "This approach makes a model based on the stopid and its previous stopids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c21bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a paired list of stops\n",
    "def paired_stops(df):\n",
    "    stopid = df['STOPPOINTID'].unique().tolist()\n",
    "    previous_stopid = []\n",
    "    for i in stopid:\n",
    "        prev = df['PREVIOUS_STOPPOINTID'][df['STOPPOINTID']==i]\n",
    "        # Adds most frequent previous stopid to list\n",
    "        previous_stopid += [prev.value_counts().idxmax()]\n",
    "    \n",
    "    return [stopid, previous_stopid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5445cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in range(len(paired_stops[0])):\n",
    "    \n",
    "    # Making new dataframe\n",
    "    to_add = df[df['STOPPOINTID']==paired_stops[0][ids]]\n",
    "    to_add = to_add.append(df[df['PREVIOUS_STOPPOINTID']==paired_stops[1][ids]])\n",
    "    stops_df = pd.DataFrame(data=to_add)\n",
    "    \n",
    "    # Setting target feature\n",
    "    y = stops_df['ACTUALTIME_TRAVEL']\n",
    "    \n",
    "    # Dropping target feature and low corr features\n",
    "    stops_df = stops_df.drop(low_corr,1)\n",
    "    stops_df = stops_df.drop('ACTUALTIME_TRAVEL',1)\n",
    "    stops_df = pd.get_dummies(stops_df)\n",
    "    \n",
    "    # Fitting model based on stops\n",
    "    linear_reg = LinearRegression().fit(stops_df, y)\n",
    "    \n",
    "    # Save to pickle file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd93db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_stops = paired_stops(df)\n",
    "to_add = df[df['STOPPOINTID']==pair_stops[0][5]]\n",
    "to_add = to_add.append(df[df['PREVIOUS_STOPPOINTID']==pair_stops[1][5]])\n",
    "stops_df = pd.DataFrame(to_add)\n",
    "\n",
    " # Setting target feature\n",
    "y = stops_df['ACTUALTIME_TRAVEL']\n",
    "    \n",
    "# Dropping target feature and low corr features\n",
    "stops_df = stops_df.drop(low_corr,1)\n",
    "stops_df = stops_df.drop('ACTUALTIME_TRAVEL',1)\n",
    "stops_df = pd.get_dummies(stops_df)\n",
    "\n",
    "# Fitting/Training model based on stops\n",
    "linear_reg_model_ = LinearRegression().fit(stops_df, y)\n",
    "\n",
    "# Saving to pickle File\n",
    "with open('model_'+pair_stops[0][5]+'.pkl', 'wb') as handle:\n",
    "    pickle.dump(linear_reg_model_, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f521b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampledf = stops_df.iloc[[0]]\n",
    "sample_prediction = linear_reg_sample.predict(sampledf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ede36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb71b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_'+pair_stops[0][5]+'.pkl', 'rb') as handle:\n",
    "    model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(sampledf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e5f48",
   "metadata": {},
   "source": [
    "## 5.2.1 Setting up for 46a stop pair models using first approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3aabfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get previous stopid and return a paired list\n",
    "def pair_stopids(current_stopids):\n",
    "    previous_stopid = []\n",
    "    for i in current_stopids:\n",
    "        prev = df['PREVIOUS_STOPPOINTID'][df['STOPPOINTID']==i]\n",
    "        # Adds most frequent previous stopid to list\n",
    "        previous_stopid += [prev.value_counts().idxmax()]\n",
    "    \n",
    "    return [current_stopids, previous_stopid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a225876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the json file\n",
    "import json\n",
    "file = open('routes_and_stops.json',)\n",
    "routes_stops = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all stops for 46a going outbound ('1')\n",
    "list_46a_stops = routes_stops['46A']['outbound']\n",
    "\n",
    "# Pairing stopids and prev stopids from 46a route\n",
    "pairing_46a_stopids = pair_stopids(list_46a_stops)\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361a2ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ids in range(len(pairing_46a_stopids[0])):\n",
    "    # Making new dataframe\n",
    "    to_add = df[df['STOPPOINTID']==pairing_46a_stopids[0][ids]]\n",
    "    to_add = to_add.append(df[df['PREVIOUS_STOPPOINTID']==pairing_46a_stopids[1][ids]])\n",
    "    stops_df = pd.DataFrame(data=to_add)\n",
    "    \n",
    "    # Setting target feature\n",
    "    y = stops_df['ACTUALTIME_TRAVEL']\n",
    "    \n",
    "    # Dropping target feature and low corr features\n",
    "    stops_df = stops_df.drop(low_corr,1)\n",
    "    stops_df = stops_df.drop('ACTUALTIME_TRAVEL',1)\n",
    "    stops_df = pd.get_dummies(stops_df)\n",
    "    \n",
    "    # Fitting model based on stops\n",
    "    linear_reg_model = LinearRegression().fit(stops_df, y)\n",
    "    \n",
    "      # Save to pickle file\n",
    "#     with open('model_'+pairing_46a_stopids[0][ids]+'.pkl', 'wb') as handle:\n",
    "#         pickle.dump(linear_reg_model, handle)\n",
    "\n",
    "     # Predicting data\n",
    "    with open('stop_'+pair_stops[0][ids]+'.pkl', 'rb') as handle:\n",
    "        model = pickle.load(handle)\n",
    "    \n",
    "    k = model.predict(route_46a.iloc[[index]])\n",
    "    predictions += [k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing evaluation metrics\n",
    "print(\"RMSE Score: \", np.sqrt(metrics.mean_squared_error(actualtimes_46a, predictions)))\n",
    "print(\"MSE Score: \", metrics.mean_squared_error(actualtimes_46a, predictions))\n",
    "print(\"MAE Score: \", metrics.mean_absolute_error(actualtimes_46a, predictions))\n",
    "print(\"R2 Score: \", metrics.r2_score(actualtimes_46a, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9191c4c2",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "##### Conclusion:\n",
    "Linear regression model is not very good. MSE score is off by more than 1000 seconds. And the R2 score is at a negative value. This means the parameters need to be tuned. Keeping dwelltime might be good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ee248",
   "metadata": {},
   "source": [
    "## 5.3 Stop pair based on entire leavetimes\n",
    "\n",
    "[Scroll to Final Stop Pair Model](#6.-Final-Stop-Pair-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3808b364",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "1) Make a function that will combine lists in a list together as one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfb17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_listsoflist(to_combine):\n",
    "    combined = []\n",
    "    for each_list in to_combine:\n",
    "        combined += each_list\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabe056",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "2) Make a function that will get rid of the duplicates in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee56935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique(stopids_list):\n",
    "    return list(set(stopids_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ed7c0c",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "3) Make a list to store all stopids for DIRECTION == outbound/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the json file\n",
    "import json\n",
    "file = open('routes_and_stops.json',)\n",
    "routes_stops = json.load(file)\n",
    "\n",
    "# Looping through every lineid, outbound \n",
    "stopids_outbound = []\n",
    "for i,j in routes_stops.items():\n",
    "    try:\n",
    "#         print(i, '\\n', routes_stops[i]['outbound'], '\\n')\n",
    "        stopids_outbound += [routes_stops[i]['outbound']]\n",
    "    except KeyError:\n",
    "        continue\n",
    "        \n",
    "# Calling function to get combined list\n",
    "combined_stopids_outbound = combine_listsoflist(stopids_outbound)\n",
    "\n",
    "# Calling function to get unique stopids from combined list\n",
    "unique_stopids_outbound = get_unique(combined_stopids_outbound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f15efd3",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "4) Make a list to store all stopids for DIRECTION ==inbound/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8584eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through every lineid, inbound\n",
    "stopids_inbound = []\n",
    "for i,j in routes_stops.items():\n",
    "    try:\n",
    "#         print(i, '\\n', routes_stops[i]['inbound'], '\\n')\n",
    "        stopids_inbound += [routes_stops[i]['inbound']]\n",
    "    except KeyError:\n",
    "        continue\n",
    "        \n",
    "# Calling function to get combined list\n",
    "combined_stopids_inbound = combine_listsoflist(stopids_inbound)\n",
    "\n",
    "# Calling function to get unique stopids from combined list - using set() to get rid off existing stops from outbound stops\n",
    "unique_stopids_inbound = list(set(combined_stopids_inbound) - set(combined_stopids_outbound))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ae717",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "3) Make a function that will get all previous stops of stops and return unique set - then make another dictionary that gets the remaining missing previous stops from routes_stops.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e5d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_previous_stopids(liststopids, direction):\n",
    "    previous_stops = {}\n",
    "    for stopid in liststopids:\n",
    "        list_toadd = []\n",
    "        for i,j in routes_stops.items():\n",
    "            try:\n",
    "                # print(i, '\\n', routes_stops[i]['outbound'], '\\n')\n",
    "                if stopid in routes_stops[i][direction]:\n",
    "                    list_stops = routes_stops[i][direction]\n",
    "                    index = list_stops.index(stopid)\n",
    "                    \n",
    "                    if index > 0:\n",
    "                        list_toadd += [list_stops[index - 1]]\n",
    "                        \n",
    "                    elif index == 0:\n",
    "                        continue\n",
    "                    \n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        previous_stops[stopid] = get_unique(list_toadd)\n",
    "        \n",
    "    return previous_stops\n",
    "\n",
    "\n",
    "sample = return_previous_stopids(unique_stopids_inbound, 'inbound')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5202ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of missing stop pairs from routes_json file\n",
    "file_1 = open('stop_pairs_inbound.json')\n",
    "prev_stops = json.load(file_1)\n",
    "\n",
    "def get_remaining(routes_stops, prev_stops):\n",
    "    to_add = {}\n",
    "    for stop in prev_stops.keys():\n",
    "        routes = sample[stop]\n",
    "        current_models = prev_stops[stop]\n",
    "        list_ = []\n",
    "        for value in routes:\n",
    "            if value not in current_models:\n",
    "                list_ += [value]\n",
    "\n",
    "        to_add[stop] = list_\n",
    "\n",
    "    # Get rid of empty keys\n",
    "    for stop in list(to_add.keys()):\n",
    "        o = to_add[stop]\n",
    "        if len(o) == 0:\n",
    "            del to_add[stop]\n",
    "            \n",
    "    return to_add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5711bd83",
   "metadata": {},
   "source": [
    "<br><br> \n",
    "4) Comparing the previous stopids available between master set (routes_stops.json) vs customed one (previous_stopids_outbound/inbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff28b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = open('previous_stops_outbound.json')\n",
    "prev_stops = json.load(file_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28edeed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make new json file for all previous stops outbound\n",
    "previous_stops_outbound = {}\n",
    "\n",
    "for stopid in unique_stopids_outbound:\n",
    "    query = \"SELECT DISTINCT PREVIOUS_STOPPOINTID from leavetimes WHERE STOPPOINTID = \" + stopid \n",
    "\n",
    "    df = pd.read_sql(query, conn)\n",
    "    list_ = df['PREVIOUS_STOPPOINTID'].tolist()\n",
    "    previous_stops_outbound[stopid] = [stopid_ for stopid_ in list_ if stopid_ != '0']\n",
    "    print('Finished, ', stopid)\n",
    "    \n",
    "with open('previous_stops_outbound.json', 'w') as fp:\n",
    "    json.dump(previous_stops_outbound, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd0971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comparing each stopid and their previous stopid\n",
    "file_1 = open('previous_stops_inbound.json')\n",
    "route_stops = sample\n",
    "outbound_stops = json.load(file_1)\n",
    "\n",
    "no_pairs = []\n",
    "final_pairs = {}\n",
    "\n",
    "for stopid in unique_stopids_inbound:\n",
    "    pairs = []\n",
    "    print('Current stopid: ', stopid)\n",
    "    \n",
    "    routes_list = route_stops[stopid]\n",
    "    outbound_list = outbound_stops[stopid]\n",
    "    \n",
    "    for element_routes in routes_list:\n",
    "        for element_outbound in outbound_list:\n",
    "            \n",
    "            if element_routes == element_outbound:\n",
    "                print('Previous stopid: ', element_routes)\n",
    "                pairs += [element_routes]\n",
    "                \n",
    "            else:\n",
    "                no_pairs += [stopid]\n",
    "    final_pairs[stopid] = pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7829241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking empty lists and adding to list\n",
    "empty_stops = []\n",
    "for key in final_pairs.keys():\n",
    "    list_ = final_pairs[key]\n",
    "    if len(list_) == 0:\n",
    "        empty_stops += [key]\n",
    "\n",
    "empty_stops = get_unique(empty_stops)\n",
    "print(len(empty_stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff613473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling empty stops with original previous stopids\n",
    "for stopid in empty_stops:\n",
    "    toadd = outbound_stops[stopid]\n",
    "    final_pairs[stopid] = toadd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20950aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Removing number 0's \n",
    "for stopid in unique_stopids_inbound:\n",
    "    if '0' in final_pairs[stopid]:\n",
    "        print('here')\n",
    "        final_pairs[stopid].remove('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d00b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stop_pairs_inbound.json', 'w') as fp:\n",
    "    json.dump(final_pairs, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5523b8",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "5) Query to select the rows based on the previous stopids and append them to the current dataframe of the current stopid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85232f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_prev_stops(query_prevstop_list):\n",
    "    query_prevstop_rows = \"SELECT leavetimes.* FROM leavetimes WHERE leavetimes.PREVIOUS_STOPPOINTID IN \" + str(query_prevstop_list)\n",
    "    df_prevstop = pd.read_sql(query_prevstop_rows, conn)\n",
    "    return df_prevstop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45edc509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_prev_stops_one_element(query_prevstop_list):\n",
    "    query_prevstop_rows = \"SELECT leavetimes.* FROM leavetimes WHERE leavetimes.PREVIOUS_STOPPOINTID = \" + str(query_prevstop_list)\n",
    "    df_prevstop = pd.read_sql(query_prevstop_rows, conn)\n",
    "    return df_prevstop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4be67c",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "6) Adding index on STOPPOINTID and PREVIOUS_STOPPOINTID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6793df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding indexes\n",
    "# add_index1 = \"\"\"CREATE INDEX stopid ON leavetimes(STOPPOINTID);\"\"\"\n",
    "# add_index2 = \"\"\"CREATE INDEX previous_stopid ON leavetimes(PREVIOUS_STOPPOINTID);\"\"\"\n",
    "# add_index3 = \"\"\"CREATE INDEX direction on trips(DIRECTION);\"\"\"\n",
    "# conn.execute(add_index1)\n",
    "# conn.execute(add_index2)\n",
    "# conn.execute(add_index3)\n",
    "# query = \"SELECT name FROM sqlite_master WHERE type = 'index';\"\n",
    "# drop = \"DROP INDEX previous_stopid\"\n",
    "# p = conn.execute(query)\n",
    "# for x in p :\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d967e8f3",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "7) Piecing every step together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe445914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists all stops done so far. This is for when laptop needs to rest\n",
    "import os \n",
    "arr = os.listdir('C:/Users/fayea/UCD/ResearchPracticum/Data-Analytics-CityRoute/stop_pair_models_inbound')\n",
    "j = []\n",
    "for i in arr:\n",
    "    j += i.split('_')\n",
    "\n",
    "h = []\n",
    "for i in j:\n",
    "    h += i.split('.')\n",
    "\n",
    "b = h[1:]\n",
    "c = b[::5]\n",
    "c = get_unique(c)\n",
    "# g = [str(i) for i in h if i.isdigit()]\n",
    "\n",
    "unique_stopids_outbound = [x for x in unique_stopids_inbound if x not in c]\n",
    "len(unique_stopids_outbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a8a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_stopids_outbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6a049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "previous_stops = {}\n",
    "for stopid in unique_stopids_outbound:\n",
    "    # Get all previous stopids in list\n",
    "    query_previoustop = \"SELECT DISTINCT leavetimes.PREVIOUS_STOPPOINTID FROM leavetimes WHERE leavetimes.STOPPOINTID = \" + stopid\n",
    "    query_prevstop_df = pd.read_sql(query_previoustop, conn)\n",
    "\n",
    "    # Converting into a pandas series then to list\n",
    "    query_prevstop_series = query_prevstop_df['PREVIOUS_STOPPOINTID'].tolist()\n",
    "    query_prevstop_list = [stopid for stopid in query_prevstop_series if stopid != '0']\n",
    "    previous_stops[stopid] = query_prevstop_list\n",
    "    print('finished')\n",
    "    \n",
    "with open('previous_stops_outbound.json', 'w+') as fp:\n",
    "    json.dump(previous_stops, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import pickle\n",
    "\n",
    "# from sagemaker import get_execution_role\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from math import log\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Connecting to s3\n",
    "# role = get_execution_role()\n",
    "# bucket='sagemaker-studio-520298385440-7in8n1t299'\n",
    "# data_key = 'route_46a.feather'\n",
    "# data_location = 's3://{}/{}'.format(bucket, data_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d487b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_corr = ['DAYOFSERVICE', 'VEHICLEID', 'TRIPID', 'STOPPOINTID', 'PREVIOUS_STOPPOINTID', 'PROGRNUMBER', 'temp', 'pressure', 'humidity', \n",
    "            'wind_speed', 'wind_deg', 'weather_id', 'weather_description', 'clouds_all', 'PREVIOUS_STOPPOINTID', 'PLANNEDTIME_ARR', 'PLANNEDTIME_DEP', 'ACTUALTIME_ARR', 'ACTUALTIME_DEP',\n",
    "           'PLANNEDTIME_TRAVEL', 'DWELLTIME', 'level_0', 'index_x', 'index_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c624b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def function to create connection to db\n",
    "def create_connection(db_file):\n",
    "    \"\"\"\n",
    "    create a database connection to the SQLite database specified by db_file\n",
    "    :param df_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try: \n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except 'Error' as e:\n",
    "        print(e)\n",
    "        \n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create connection to db\n",
    "db_file = \"C:/Users/fayea/UCD/ResearchPracticum/Data-Analytics-CityRoute/dublinbus.db\"\n",
    "conn = create_connection(db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a0341f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Outbound\n",
    "file = open('previous_stops_outbound.json',)\n",
    "previous_stops = json.load(file)\n",
    "\n",
    "# Master set of features\n",
    "dummies_features = {'MONTHOFSERVICE_April': 0, 'MONTHOFSERVICE_August': 0, 'MONTHOFSERVICE_December': 0, 'MONTHOFSERVICE_February': 0, 'MONTHOFSERVICE_January': 0, 'MONTHOFSERVICE_July': 0, \n",
    "                'MONTHOFSERVICE_June': 0, 'MONTHOFSERVICE_March': 0, 'MONTHOFSERVICE_May': 0, 'MONTHOFSERVICE_November': 0, 'MONTHOFSERVICE_October': 0, 'MONTHOFSERVICE_September': 0,\n",
    "                'DAYOFWEEK_Friday': 0, 'DAYOFWEEK_Monday': 0, 'DAYOFWEEK_Thursday': 0,'DAYOFWEEK_Tuesday': 0, 'DAYOFWEEK_Wednesday': 0,\n",
    "                'DAYOFWEEK_Saturday': 0, 'DAYOFWEEK_Sunday': 0, 'weather_main_Clouds': 0, 'weather_main_Drizzle': 0, \n",
    "                'weather_main_Fog': 0, 'weather_main_Mist': 0,'weather_main_Rain': 0, 'weather_main_Snow': 0,  'weather_main_Clear': 0, 'rain_1h': 0,\n",
    "                'IS_HOLIDAY_0': 0, 'IS_WEEKDAY_1': 0, 'IS_WEEKDAY_0': 0, 'IS_HOLIDAY_1': 0, 'eve_rushour_1': 0, 'eve_rushour_0': 0, 'morn_rushour_0': 0, 'morn_rushour_1': 0}\n",
    "\n",
    "dummy_keys = ['rain_1h', 'MONTHOFSERVICE_April', 'MONTHOFSERVICE_August',\n",
    "       'MONTHOFSERVICE_December', 'MONTHOFSERVICE_February',\n",
    "       'MONTHOFSERVICE_January', 'MONTHOFSERVICE_July', 'MONTHOFSERVICE_June',\n",
    "       'MONTHOFSERVICE_March', 'MONTHOFSERVICE_May', 'MONTHOFSERVICE_November',\n",
    "       'MONTHOFSERVICE_October', 'MONTHOFSERVICE_September',\n",
    "       'DAYOFWEEK_Friday', 'DAYOFWEEK_Monday', 'DAYOFWEEK_Saturday',\n",
    "       'DAYOFWEEK_Sunday', 'DAYOFWEEK_Thursday', 'DAYOFWEEK_Tuesday',\n",
    "       'DAYOFWEEK_Wednesday', 'IS_HOLIDAY_0', 'IS_HOLIDAY_1', 'IS_WEEKDAY_0',\n",
    "       'IS_WEEKDAY_1', 'weather_main_Clear', 'weather_main_Clouds',\n",
    "       'weather_main_Drizzle', 'weather_main_Fog', 'weather_main_Mist',\n",
    "       'weather_main_Rain', 'weather_main_Snow', 'eve_rushour_1', 'eve_rushour_0', 'morn_rushour_0', 'morn_rushour_1']\n",
    "\n",
    "# Query to get all of weather\n",
    "weather_query = \"SELECT weather.* from weather\"\n",
    "weather_df = pd.read_sql(weather_query, conn)\n",
    "weather_df = weather_df.rename(columns={\"dt\": \"DAYOFSERVICE\"})\n",
    "\n",
    "low_corr = ['DAYOFSERVICE', 'VEHICLEID', 'TRIPID', 'STOPPOINTID', 'PREVIOUS_STOPPOINTID', 'PROGRNUMBER', 'temp', 'pressure', 'humidity', \n",
    "            'wind_speed', 'wind_deg', 'weather_id', 'weather_description', 'clouds_all', 'PREVIOUS_STOPPOINTID', 'PLANNEDTIME_ARR', 'PLANNEDTIME_DEP', 'ACTUALTIME_ARR', 'ACTUALTIME_DEP',\n",
    "           'PLANNEDTIME_TRAVEL', 'level_0', 'index_x', 'index_y']\n",
    "\n",
    "index = 0\n",
    "\n",
    "for current_stopid in to_add:\n",
    "    \n",
    "    query_prevstop_series = previous_stops[current_stopid]\n",
    "    query_prevstop_list = tuple(query_prevstop_series)\n",
    "    \n",
    "    if len(query_prevstop_list) == 1:\n",
    "        # Making query to db and make df\n",
    "        query_stopid = \"SELECT leavetimes.* FROM leavetimes WHERE leavetimes.STOPPOINTID = \" + current_stopid\n",
    "        df = pd.read_sql(query_stopid, conn)\n",
    "        \n",
    "        # Append previous stops rows to main df\n",
    "        to_add = df_prev_stops_one_element(query_prevstop_series[0])\n",
    "        df = pd.concat([df,to_add])\n",
    "        df = df.merge(weather_df, on='DAYOFSERVICE', how='left')\n",
    "        \n",
    "    elif len(query_prevstop_list) == 0:\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        # Making query to db and make df\n",
    "        query_stopid = \"SELECT leavetimes.* FROM leavetimes WHERE leavetimes.STOPPOINTID = \" + current_stopid \n",
    "        df = pd.read_sql(query_stopid, conn)\n",
    "\n",
    "        # Append previous stops rows to main df\n",
    "        to_add = df_prev_stops(query_prevstop_list)\n",
    "        df = pd.concat([df,to_add]) \n",
    "        df = df.merge(weather_df, on='DAYOFSERVICE', how='left')\n",
    "        \n",
    "    # Drop low correlated features and setting target feature\n",
    "    df = df.drop(low_corr, 1)\n",
    "    \n",
    "    df['IS_HOLIDAY'] = df['IS_HOLIDAY'].astype('category')\n",
    "    df['IS_WEEKDAY'] = df['IS_WEEKDAY'].astype('category')\n",
    "    \n",
    "    tf = df['ACTUALTIME_TRAVEL']\n",
    "    \n",
    "    df = df.drop('ACTUALTIME_TRAVEL', 1)\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    if df.shape[1] < 31:\n",
    "        for key in dummy_keys:\n",
    "            if dummies_features[key] not in df.columns:\n",
    "                df[key] = dummies_features[key]\n",
    "    # Fitting model based on stops\n",
    "    linear_reg_model = LinearRegression().fit(df, tf)\n",
    "    \n",
    "    # Save to pickle file\n",
    "    with open('C:/Users/fayea/UCD/ResearchPracticum/Data-Analytics-CityRoute/stop_models_outbound/stop_'+ current_stopid +'.pkl', 'wb') as handle:\n",
    "        pickle.dump(linear_reg_model, handle)\n",
    "    \n",
    "    print(current_stopid, df.shape[1], index, ' Finished.')\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db6c39e",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "# 6. Final Stop Pair Model\n",
    "[Back to Top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b84ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from math import log\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bda7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    \"\"\"\n",
    "    create a database connection to the SQLite database specified by db_file\n",
    "    :param df_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try: \n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except 'Error' as e:\n",
    "        print(e)\n",
    "        \n",
    "    return conn\n",
    "\n",
    "# create connection to db\n",
    "db_file = \"C:/Users/fayea/UCD/ResearchPracticum/Data-Analytics-CityRoute/dublinbus.db\"\n",
    "conn = create_connection(db_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0eb4ce",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "[Load functions](#5.3-Stop-pair-based-on-entire-leavetimes)\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master set of features\n",
    "dummies_features = {'MONTHOFSERVICE_April': 0, 'MONTHOFSERVICE_August': 0, 'MONTHOFSERVICE_December': 0, 'MONTHOFSERVICE_February': 0, 'MONTHOFSERVICE_January': 0, 'MONTHOFSERVICE_July': 0, \n",
    "                'MONTHOFSERVICE_June': 0, 'MONTHOFSERVICE_March': 0, 'MONTHOFSERVICE_May': 0, 'MONTHOFSERVICE_November': 0, 'MONTHOFSERVICE_October': 0, 'MONTHOFSERVICE_September': 0,\n",
    "                'DAYOFWEEK_Friday': 0, 'DAYOFWEEK_Monday': 0, 'DAYOFWEEK_Thursday': 0,'DAYOFWEEK_Tuesday': 0, 'DAYOFWEEK_Wednesday': 0,\n",
    "                'DAYOFWEEK_Saturday': 0, 'DAYOFWEEK_Sunday': 0, 'weather_main_Clouds': 0, 'weather_main_Drizzle': 0, \n",
    "                'weather_main_Fog': 0, 'weather_main_Mist': 0,'weather_main_Rain': 0, 'weather_main_Snow': 0,  'weather_main_Clear': 0, 'rain_1h': 0,\n",
    "                'IS_HOLIDAY_0': 0, 'IS_WEEKDAY_1': 0, 'IS_WEEKDAY_0': 0, 'IS_HOLIDAY_1': 0, 'eve_rushour_1': 0, 'eve_rushour_0': 0, 'morn_rushour_0': 0, 'morn_rushour_1': 0}\n",
    "\n",
    "dummy_keys = ['rain_1h', 'MONTHOFSERVICE_April', 'MONTHOFSERVICE_August',\n",
    "       'MONTHOFSERVICE_December', 'MONTHOFSERVICE_February',\n",
    "       'MONTHOFSERVICE_January', 'MONTHOFSERVICE_July', 'MONTHOFSERVICE_June',\n",
    "       'MONTHOFSERVICE_March', 'MONTHOFSERVICE_May', 'MONTHOFSERVICE_November',\n",
    "       'MONTHOFSERVICE_October', 'MONTHOFSERVICE_September',\n",
    "       'DAYOFWEEK_Friday', 'DAYOFWEEK_Monday', 'DAYOFWEEK_Saturday',\n",
    "       'DAYOFWEEK_Sunday', 'DAYOFWEEK_Thursday', 'DAYOFWEEK_Tuesday',\n",
    "       'DAYOFWEEK_Wednesday', 'IS_HOLIDAY_0', 'IS_HOLIDAY_1', 'IS_WEEKDAY_0',\n",
    "       'IS_WEEKDAY_1', 'weather_main_Clear', 'weather_main_Clouds',\n",
    "       'weather_main_Drizzle', 'weather_main_Fog', 'weather_main_Mist',\n",
    "       'weather_main_Rain', 'weather_main_Snow', 'eve_rushour_1', 'eve_rushour_0', 'morn_rushour_0', 'morn_rushour_1']\n",
    "\n",
    "# Query to get all of weather\n",
    "weather_query = \"SELECT weather.* from weather\"\n",
    "weather_df = pd.read_sql(weather_query, conn)\n",
    "weather_df = weather_df.rename(columns={\"dt\": \"DAYOFSERVICE\"})\n",
    "\n",
    "low_corr = ['DAYOFSERVICE', 'VEHICLEID', 'TRIPID', 'STOPPOINTID', 'PREVIOUS_STOPPOINTID', 'PROGRNUMBER', 'temp', 'pressure', 'humidity', \n",
    "            'wind_speed', 'wind_deg', 'weather_id', 'weather_description', 'clouds_all', 'PREVIOUS_STOPPOINTID', 'PLANNEDTIME_ARR', 'PLANNEDTIME_DEP', 'ACTUALTIME_ARR', 'ACTUALTIME_DEP',\n",
    "           'PLANNEDTIME_TRAVEL', 'level_0', 'index_x', 'index_y', 'index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82676cc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "file = open('stop_pairs_outbound.json',)\n",
    "previous_stops = json.load(file)\n",
    "# f = pd.DataFrame()\n",
    "\n",
    "for current_stopid in unique_stopids_outbound:\n",
    "#     print(current_stopid)\n",
    "    previous_stops_list = previous_stops[str(current_stopid)]\n",
    "\n",
    "    if len(previous_stops_list) > 0:\n",
    "        query_stopid = \"SELECT leavetimes.* FROM leavetimes WHERE leavetimes.STOPPOINTID = \" + current_stopid\n",
    "        df = pd.read_sql(query_stopid, conn)\n",
    "        \n",
    "        \n",
    "        # Adding Extra Features\n",
    "        df['eve_rushour'] = [1 if int(time.strftime(\"%H\",time.gmtime(hour))) >= 16 and int(time.strftime(\"%H\",time.gmtime(hour))) <= 19 else 0 for hour in df['ACTUALTIME_DEP']]\n",
    "        df['morn_rushour'] = [1 if int(time.strftime(\"%H\",time.gmtime(hour))) >= 7 and int(time.strftime(\"%H\",time.gmtime(hour))) <= 9 else 0 for hour in df['ACTUALTIME_DEP']]\n",
    "        df['morn_rushour'] = df['morn_rushour'].astype('category')\n",
    "        df['eve_rushour'] = df['eve_rushour'].astype('category')\n",
    "        df['IS_HOLIDAY'] = df['IS_HOLIDAY'].astype('category')\n",
    "        df['IS_WEEKDAY'] = df['IS_WEEKDAY'].astype('category')\n",
    "        \n",
    "        df = df.merge(weather_df, on='DAYOFSERVICE', how='left')\n",
    "#         f = f.append(df)\n",
    "                    \n",
    "        for previous_stop in previous_stops_list:\n",
    "            new_df = df[df['PREVIOUS_STOPPOINTID']==previous_stop]\n",
    "#             f = f.append(new_df)\n",
    "        \n",
    "            \n",
    "            tf = new_df['ACTUALTIME_TRAVEL']\n",
    "            new_df = new_df.drop('ACTUALTIME_TRAVEL', 1)\n",
    "            new_df = new_df.drop(low_corr, 1)\n",
    "            new_df = pd.get_dummies(new_df)\n",
    "            \n",
    "            if new_df.shape[1] < 36:\n",
    "                for key in dummy_keys:\n",
    "                    if dummies_features[key] not in new_df.columns:\n",
    "                        new_df[key] = dummies_features[key]\n",
    "                \n",
    "            # Fitting model based on stops\n",
    "            linear_reg_model = LinearRegression().fit(new_df, tf)\n",
    "            \n",
    "            # Save to pickle file\n",
    "            with open('C:/Users/fayea/UCD/ResearchPracticum/Data-Analytics-CityRoute/stop_pair_models_outbound/stop_'+ current_stopid +'_' + previous_stop + '_outbound.pkl', 'wb') as handle:\n",
    "                pickle.dump(linear_reg_model, handle)\n",
    "                \n",
    "            print(current_stopid, previous_stop, new_df.shape[1], index, ' Finished.')\n",
    "    \n",
    "            \n",
    "    elif len(previous_stops_list) == 0:\n",
    "#         print('here')\n",
    "        continue\n",
    "        \n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cbd61e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "file = open('stop_pairs_outbound.json',)\n",
    "previous_stops = json.load(file)\n",
    "f = pd.DataFrame()\n",
    "\n",
    "for current_stopid in list(to_add.keys()):\n",
    "    \n",
    "    previous_stops_list = to_add[current_stopid]\n",
    "\n",
    "    if len(previous_stops_list) > 0:\n",
    "        query_stopid = \"SELECT leavetimes.* FROM leavetimes WHERE leavetimes.STOPPOINTID = \" + current_stopid\n",
    "        df = pd.read_sql(query_stopid, conn)\n",
    "                    \n",
    "        for previous_stop in previous_stops_list:\n",
    "            \n",
    "            previous_stop_query =  \"SELECT leavetimes.* FROM leavetimes WHERE leavetimes.PREVIOUS_STOPPOINTID = \" + previous_stop\n",
    "            prev_stop_df = pd.read_sql(query_stopid, conn)\n",
    "            \n",
    "            # Append previous stops rows to main df\n",
    "            new_df = pd.concat([df,prev_stop_df])\n",
    "            new_df = new_df.merge(weather_df, on='DAYOFSERVICE', how='left')\n",
    "            \n",
    "            # Adding Extra Features\n",
    "            new_df['eve_rushour'] = [1 if int(time.strftime(\"%H\",time.gmtime(hour))) >= 16 and int(time.strftime(\"%H\",time.gmtime(hour))) <= 19 else 0 for hour in new_df['ACTUALTIME_DEP']]\n",
    "            new_df['morn_rushour'] = [1 if int(time.strftime(\"%H\",time.gmtime(hour))) >= 7 and int(time.strftime(\"%H\",time.gmtime(hour))) <= 9 else 0 for hour in new_df['ACTUALTIME_DEP']]\n",
    "            new_df['morn_rushour'] = new_df['morn_rushour'].astype('category')\n",
    "            new_df['eve_rushour'] = new_df['eve_rushour'].astype('category')\n",
    "            new_df['IS_HOLIDAY'] = new_df['IS_HOLIDAY'].astype('category')\n",
    "            new_df['IS_WEEKDAY'] = new_df['IS_WEEKDAY'].astype('category')\n",
    "            \n",
    "            tf = new_df['ACTUALTIME_TRAVEL']\n",
    "            new_df = new_df.drop('ACTUALTIME_TRAVEL', 1)\n",
    "            new_df = new_df.drop(low_corr, 1)\n",
    "            new_df = pd.get_dummies(new_df)\n",
    "#             f = f.append(new_df)\n",
    "            if new_df.shape[1] < 36:\n",
    "                for key in dummy_keys:\n",
    "                    if dummies_features[key] not in new_df.columns:\n",
    "                        new_df[key] = dummies_features[key]\n",
    "                        \n",
    "            # Fitting model based on stops\n",
    "            linear_reg_model = LinearRegression().fit(new_df, tf)\n",
    "            \n",
    "            # Save to pickle file\n",
    "            with open('C:/Users/fayea/UCD/ResearchPracticum/Data-Analytics-CityRoute/stop_pair_models_outbound/stop_'+ current_stopid +'_' + previous_stop + '_outbound.pkl', 'wb') as handle:\n",
    "                pickle.dump(linear_reg_model, handle)\n",
    "                \n",
    "            print(current_stopid, previous_stop, new_df.shape[1], index, ' Finished.')\n",
    "    \n",
    "            \n",
    "    elif len(previous_stops_list) == 0:\n",
    "        continue\n",
    "        \n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c12bd",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
